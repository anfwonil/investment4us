# app.py
# pip install -U streamlit pandas plotly yfinance xlsxwriter requests beautifulsoup4 lxml

import os, re, math, time
from io import BytesIO
from datetime import date
import html as ihtml
from lxml import etree

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
import yfinance as yf
import requests
from io import StringIO


st.set_page_config(page_title="Market Performance", layout="wide")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_CSV = os.path.join(BASE_DIR, "data", "market_timeseries.csv")
META_CSV  = os.path.join(BASE_DIR, "data", "meta.csv")

# -------------------- ì¸ì¦(ì„ íƒ) --------------------
def get_app_password():
    try:
        return st.secrets["APP_PASSWORD"]
    except Exception:
        return os.getenv("APP_PASSWORD", "")

APP_PW = get_app_password()
if APP_PW:
    pw = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if pw != APP_PW:
        st.stop()

# ---------- í€ë“œ ë§¤í•‘: ë¡œë“œ & ê²€ìƒ‰ ìœ í‹¸ ----------
FUND_MAP_PATHS = [
    os.path.join(BASE_DIR, "data", "fundcodematching.xlsx"),  # ê¶Œì¥
    os.path.join(BASE_DIR, "data", "fundcodematching.xls"),    # êµ¬ë²„ì „
    os.path.join(BASE_DIR, "fundcodematching.xlsx"),
    os.path.join(BASE_DIR, "fundcodematching.xls"),
]

def _first_existing(paths):
    for p in paths:
        if os.path.exists(p):
            return p
    return None

def _truncate20(s: str) -> str:
    s = str(s).strip()
    return (s[:20] + "â€¦") if len(s) > 20 else s

def _norm(s: str) -> str:
    # ê³µë°±/êµ¬ë‘ì  ì œê±° + ì†Œë¬¸ìí™” (í•œê¸€ í¬í•¨ ë‹¨ìˆœ ë¶€ë¶„ì¼ì¹˜ìš©)
    import re
    return re.sub(r"[\s\W_]+", "", str(s)).lower()

@st.cache_data(ttl=600, show_spinner=False)
def load_fund_map():
    """
    ì—‘ì…€/CSVì—ì„œ (ì½”ë“œ, í€ë“œëª…) ë§¤í•‘ ë¡œë“œ.
    ì§€ì› ì»¬ëŸ¼ëª…(ëŒ€ì†Œë¬¸ì ë¬´ê´€):
      - ì½”ë“œ: 'code','fund_code','í€ë“œì½”ë“œ','ì½”ë“œ'
      - ì´ë¦„: 'name','fund_name','í€ë“œëª…','ì´ë¦„'
    """
    path = _first_existing(FUND_MAP_PATHS)
    if not path:
        return pd.DataFrame(columns=["code","name","name20","norm_name"])

    # í™•ì¥ìì— ë”°ë¼ ì½ê¸°
    ext = os.path.splitext(path)[1].lower()
    if ext in (".xlsx",):
        df = pd.read_excel(path, engine="openpyxl")
    elif ext in (".xls",):
        # xlrdê°€ í•„ìš”í•©ë‹ˆë‹¤(ë¡œì»¬ requirementsì— ì´ë¯¸ í¬í•¨í•˜ì‹  ê²ƒìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤)
        df = pd.read_excel(path)  # engine ìë™
    else:
        df = pd.read_csv(path)

    # ì»¬ëŸ¼ëª… ì •ê·œí™”
    cols = {c.lower(): c for c in df.columns}
    code_col = next((cols[k] for k in cols if k in ("code","fund_code","í€ë“œì½”ë“œ","ì½”ë“œ")), None)
    name_col = next((cols[k] for k in cols if k in ("name","fund_name","í€ë“œëª…","ì´ë¦„")), None)
    if not code_col or not name_col:
        # ì•ˆì „ì¥ì¹˜: Aì—´=ì½”ë“œ, Bì—´=í€ë“œëª… ê°€ì •
        df = df.copy()
        df.columns = [f"col{i+1}" for i in range(len(df.columns))]
        name_col, code_col = "col1","col2"   # A=í€ë“œëª…, B=ì½”ë“œ

    out = df[[code_col, name_col]].rename(columns={code_col:"code", name_col:"name"}).copy()
    out["code"] = out["code"].astype(str).str.strip().str.upper()
    out["name"] = out["name"].astype(str).str.strip()
    out = out.dropna(subset=["code","name"]).drop_duplicates(subset=["code"])
    out["name20"] = out["name"].map(_truncate20)
    out["norm_name"] = out["name"].map(_norm)
    return out

FUND_MAP = load_fund_map()
FUND_CODE_SET = set(FUND_MAP["code"]) if not FUND_MAP.empty else set()

def resolve_terms_to_codes(terms: list[str]) -> tuple[list[str], dict]:
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ í† í° ëª©ë¡ì„ â†’ (í™•ì • ì½”ë“œ ë¦¬ìŠ¤íŠ¸, ë¼ë²¨ë§µ)ìœ¼ë¡œ ë³€í™˜
    - í† í°ì´ ì´ë¯¸ í‹°ì»¤(ì½”ë“œ)ì´ë©´ ê·¸ëŒ€ë¡œ
    - ì•„ë‹ˆë©´ í€ë“œëª… ë¶€ë¶„ì¼ì¹˜ ê²€ìƒ‰ìœ¼ë¡œ codeë¥¼ ì°¾ì•„ ì¶”ê°€
    ë¼ë²¨ë§µ: {ì½”ë“œ: í‘œì‹œëª…(20ì ì ˆë‹¨)}
    """
    codes = []
    label_map = {}

    # ì‚¬ì „ìƒì„±: norm_name -> code (ë‹¤ëŒ€ì¼ í—ˆìš© ìœ„í•´ ê²€ìƒ‰ì€ containsë¡œ)
    for t in terms:
        if not t:
            continue
        t_clean = t.strip()
        # ì´ë¯¸ ì½”ë“œë¡œ ë“¤ì–´ì˜¨ ê²½ìš° (ëŒ€ë¬¸ì/ìˆ«ì/ê¸°í˜¸ í˜¼í•© í—ˆìš©)
        if t_clean.upper() in FUND_CODE_SET:
            codes.append(t_clean.upper())
            # label ì¤€ë¹„
            row = FUND_MAP.loc[FUND_MAP["code"]==t_clean.upper()].head(1)
            if not row.empty:
                nm = row.iloc[0]["name20"]
                label_map[t_clean.upper()] = nm
            continue

        # ì´ë¦„ ê²€ìƒ‰ (ë¶€ë¶„ì¼ì¹˜)
        tnorm = _norm(t_clean)
        hits = FUND_MAP.loc[FUND_MAP["norm_name"].str.contains(tnorm, na=False)]
        if not hits.empty:
            for code, nm20 in zip(hits["code"], hits["name20"]):
                codes.append(code)
                label_map[code] = nm20

    # ì¤‘ë³µ ì œê±°(ì…ë ¥ ìˆœì„œ ë³´ì¡´)
    seen = set()
    uniq = []
    for c in codes:
        if c not in seen:
            uniq.append(c)
            seen.add(c)

    return uniq, label_map

import yfinance as yf

def search_ticker_or_fund(query: str):
    q = query.strip().upper()
    if not q:
        return []

    # 1. ì•¼í›„ í‹°ì»¤ ì‹œë„
    try:
        test = yf.Ticker(q)
        hist = test.history(period="1d")
        if not hist.empty:
            return [q]   # âœ… ì•¼í›„ì—ì„œ ë°”ë¡œ ë°ì´í„° ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    except Exception:
        pass

    # 2. í€ë“œ ë§¤í•‘ ê²€ìƒ‰
    matches_code = FUND_MAP.loc[FUND_MAP["code"].str.upper() == q]
    matches_name = FUND_MAP.loc[FUND_MAP["name"].str.upper() == q]
    if not matches_code.empty or not matches_name.empty:
        return list(matches_code["code"]) + list(matches_name["code"])

    # 3. ë¶€ë¶„ê²€ìƒ‰ (ë„ˆë¬´ ì§§ì€ ê±´ ìŠ¤í‚µ)
    if len(q) > 2:
        hits = FUND_MAP.loc[FUND_MAP["norm_name"].str.contains(q.lower(), na=False)]
        return list(hits["code"])

    return []


def pretty_label_with_fund(code_or_ticker: str) -> str:
    """
    ê·¸ë˜í”„ ë¼ë²¨ì— ì‚¬ìš©í•  í‘œì‹œëª…:
    - FUND_MAPì— ìˆìœ¼ë©´ í€ë“œëª…(20ì)
    - ì—†ìœ¼ë©´ ê¸°ì¡´ pretty_label(code_or_ticker) ê²°ê³¼ ì‚¬ìš©
    """
    c = str(code_or_ticker).upper()
    hit = FUND_MAP.loc[FUND_MAP["code"]==c]
    if not hit.empty:
        return hit.iloc[0]["name20"]
    # (ê¸°ì¡´ pretty_label ì´ ì´ë¯¸ ìˆë‹¤ë©´ ê·¸ê±¸ í˜¸ì¶œí•˜ì„¸ìš”)
    try:
        return pretty_label(c)  # ë‹¹ì‹ ì˜ ê¸°ì¡´ í•¨ìˆ˜
    except Exception:
        return c
    
    def label_of(sym: str) -> str:
        s = str(sym).strip().upper()
        if not FUND_MAP.empty and "code" in FUND_MAP.columns:
            hit = FUND_MAP.loc[FUND_MAP["code"] == s]
            if not hit.empty:
                return hit.iloc[0]["name20"]  # 20ì ì ˆë‹¨ í€ë“œëª…
        return pretty_label(s)

def _is_krx_symbol(s: str) -> bool:
    """6ìë¦¬(ì„ íƒì ìœ¼ë¡œ .KS/.KQ) í•œêµ­ê±°ë˜ì†Œ ì‹¬ë³¼ íŒë³„"""
    s = str(s).strip().upper()
    return bool(re.fullmatch(r"\d{6}(\.K[QS])?", s))

def _to_krx_code(s: str) -> str | None:
    """'005930' ë˜ëŠ” '005930.KS' â†’ '005930'ë¡œ í‘œì¤€í™”, ì•„ë‹ˆë©´ None"""
    m = re.fullmatch(r"(\d{6})(?:\.K[QS])?$", str(s).strip().upper())
    return m.group(1) if m else None


# ==== KOFIA(DIS) XML endpoint helpers ========================================
KOFIA_URL = "https://dis.kofia.or.kr/proframeWeb/XMLSERVICES/"

def _truncate_bytes(s: str, max_bytes: int = 15, encoding="utf-8") -> str:
    b = s.encode(encoding)[:max_bytes]
    while True:
        try:
            return b.decode(encoding)
        except UnicodeDecodeError:
            b = b[:-1]

def is_kofia_code(tok: str) -> bool:
    """í€ë“œ ì½”ë“œ íŒë³„: 
       - êµ­ë‚´ ISIN: KR + 10 digits (ì˜ˆ: KR5370199261)
       - ë‹¨ì¶•ì½”ë“œ: 4~6 digits (ì˜ˆ: 19926)
       - ì¼ë¶€ ìš´ìš©ì‚¬ ì „ìš© ì½”ë“œ: K + 11 alnum (ì˜ˆ: K55235B39924)
    """
    if not tok:
        return False
    t = tok.strip().upper()
    return bool(re.fullmatch(r"(KR\d{10}|K[A-Z0-9]{11}|\d{4,6})", t))


# === ê³µìš© ë‚ ì§œ íŒŒì„œ(YYYYMMDD ìš°ì„ , ê·¸ ì™¸ í¬ë§·ë„ ìˆ˜ìš©) ===
def _parse_date_series(s: pd.Series) -> pd.Series:
    """
    1) 8ìë¦¬ YYYYMMDD ìš°ì„  ì‹œë„
    2) êµ¬ë¶„ì í†µì¼ í›„ ììœ  íŒŒì‹±
    3) ì—¬ì „íˆ Noneì´ë©´ YYYYMMDD ì¬ì‹œë„
    â€» infer_datetime_format ì˜µì…˜ ì œê±°(ì‹ ë²„ì „ ê²½ê³  ë°©ì§€)
    """
    raw = s.astype(str).str.strip()

    # 1) YYYYMMDD ì§ì ‘ íŒŒì‹±
    dt_a = pd.to_datetime(raw, errors="coerce", format="%Y%m%d")
    if dt_a.notna().sum() >= max(3, int(len(raw) * 0.3)):
        return dt_a

    # 2) êµ¬ë¶„ì í†µì¼ + íŒ¨í„´ ì¶”ì¶œ í›„ íŒŒì‹±
    norm = (raw.str.replace(".", "-", regex=False)
                .str.replace("/", "-", regex=False))
    # 'YYYY-MM-DD' íŒ¨í„´ ìš°ì„  ì¶”ì¶œ, ì—†ìœ¼ë©´ ì›ë¬¸
    norm2 = norm.str.extract(r"(\d{4}-?\d{2}-?\d{2})", expand=False).fillna(norm)
    dt_b = pd.to_datetime(norm2, errors="coerce")

    # 3) ìˆ«ìë§Œ ë‚¨ê²¨ YYYYMMDD ì¬ì‹œë„
    norm3 = norm2.str.replace("-", "", regex=False)
    dt_c = pd.to_datetime(norm3, errors="coerce", format="%Y%m%d")

    return dt_b.where(dt_b.notna(), dt_c)

def _kofia_xml_payload(start_ymd: str, end_ymd: str, fund_id: str) -> str:
    return f"""<?xml version="1.0" encoding="utf-8"?>
<message>
  <proframeHeader>
    <pfmAppName>FS-DIS2</pfmAppName>
    <pfmSvcName>DISFundStdPrcStutSO</pfmSvcName>
    <pfmFnName>select</pfmFnName>
  </proframeHeader>
  <systemHeader></systemHeader>
  <DISCondFuncDTO>
    <tmpV30>{start_ymd}</tmpV30>
    <tmpV31>{end_ymd}</tmpV31>
    <tmpV10>0</tmpV10>
    <tmpV12>{fund_id}</tmpV12>
  </DISCondFuncDTO>
</message>""".strip()

# --- (NEW) KOFIA XMLì—ì„œ í€ë“œëª…ì„ ì–´ë””ì— ìˆì–´ë„ ì°¾ì•„ë‚´ê¸° ---
def _extract_kofia_name_from_xml(xml_bytes: bytes) -> str | None:
    try:
        root = etree.fromstring(xml_bytes)
    except Exception:
        return None

    # 1) ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ íƒœê·¸ë“¤ ìš°ì„ 
    name_tags = [
        "fundNm", "fundName",
        "fnccFundNm", "fnccFundNm1", "fnccFundNm2",
        "vGridBtn1", "vGridBtn2", "ext1", "ext2", "tmpV5", "tmpV4"
    ]
    for tag in name_tags:
        vals = root.xpath(f'//*[local-name()="{tag}"]/text()')
        for v in vals:
            s = (v or "").strip()
            if not s:
                continue
            # vGridBtn1 ë“±ì€ <a>íƒœê·¸ê°€ ì„ì—¬ ì˜¬ ìˆ˜ ìˆìœ¼ë‹ˆ íƒœê·¸ ì œê±°
            s = re.sub(r"<[^>]*>", "", s)
            s = re.sub(r"\s+", " ", s).strip()
            if s:
                return s

    # 2) í˜¹ì‹œ ì†ì„±(attribute)ì— ì´ë¦„ì´ ë“¤ì–´ì˜¤ëŠ” ì¼€ì´ìŠ¤ë„ ë°©ì–´
    cand_nodes = root.xpath('//*[@*]')
    for node in cand_nodes:
        for _, attr_val in getattr(node, 'attrib', {}).items():
            s = re.sub(r"<[^>]*>", "", (attr_val or "")).strip()
            if s and len(s) >= 3 and not s.isdigit():
                return s

    return None


@st.cache_data(ttl=3600, show_spinner=False)
def fetch_kofia_nav_xml(code: str, start, end) -> pd.DataFrame:
    """
    KOFIA(DIS)ì—ì„œ í€ë“œ ê¸°ì¤€ê°€(NAV)ë¥¼ ['Date', code, (optional) 'fundNm']ë¡œ ë°˜í™˜.
      - Date  : tmpV1 (YYYYMMDD)
      - Price : tmpV2 (ê¸°ì¤€ê°€ê²©, ì›)
      - Name  : XML ì–´ë””ì— ìˆë“  ìµœëŒ€í•œ ì°¾ì•„ 'fundNm'ë¡œ ë™ë´‰
    """
    code = (code or "").strip()
    if not code:
        return pd.DataFrame(columns=["Date"])

    def _yyyymmdd(d): return pd.Timestamp(d).strftime("%Y%m%d")
    payload = _kofia_xml_payload(_yyyymmdd(start), _yyyymmdd(end), code)
    headers = {
        "Content-Type": "text/xml; charset=UTF-8",
        "Accept": "text/xml,application/xml,text/plain,*/*",
        "User-Agent": "Mozilla/5.0",
    }

    # --- ìš”ì²­ ---
    try:
        r = requests.post(KOFIA_URL, data=payload.encode("utf-8"), headers=headers, timeout=20)
        if not r.ok:
            st.warning(f"KOFIA HTTP ì˜¤ë¥˜({code}): {r.status_code}")
            return pd.DataFrame(columns=["Date"])
        xml_bytes = r.content
    except Exception as e:
        st.warning(f"KOFIA ìš”ì²­ ì‹¤íŒ¨({code}): {e}")
        return pd.DataFrame(columns=["Date"])

    # --- íŒŒì‹± ---
    try:
        root = etree.fromstring(xml_bytes)
        nodes = root.xpath('//*[local-name()="DISCondFuncListDTO"]/*')
        if not nodes:
            nodes = root.xpath('//*[local-name()="DISCondFuncDTO"]')

        rows = []
        for n in nodes:
            row = {}
            for c in n.iterchildren():
                try:
                    k = etree.QName(c.tag).localname
                except Exception:
                    k = (str(c.tag).split("}")[-1] if "}" in str(c.tag) else str(c.tag))
                row[k] = (c.text or "").strip()
            if any(v for v in row.values()):
                rows.append(row)
        if not rows:
            return pd.DataFrame(columns=["Date"])

        df = pd.DataFrame(rows).reset_index(drop=True)

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        if "tmpV1" not in df.columns or "tmpV2" not in df.columns:
            st.warning(f"KOFIA ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ë³€ê²½ ê°€ëŠ¥ì„±({code}). cols={list(df.columns)}")
            return pd.DataFrame(columns=["Date"])

        # ë‚ ì§œ/ê°€ê²©
        dates = pd.to_datetime(df["tmpV1"].astype(str), errors="coerce", format="%Y%m%d")
        price = pd.to_numeric(
            df["tmpV2"].astype(str).str.replace(",", "", regex=False).str.replace(" ", "", regex=False),
            errors="coerce"
        )

        # --- (í•µì‹¬) í€ë“œëª…ì€ ë¬¸ì„œ ì „ì²´ì—ì„œ ìŠ¤ìº” ---
        fund_name = None
        # 1ìˆœìœ„: ëª…ì‹œì  ì»¬ëŸ¼
        if "fundNm" in df.columns and df["fundNm"].notna().any():
            fund_name = str(df["fundNm"].dropna().iloc[0]).strip()
        # 2ìˆœìœ„: ë¬¸ì„œ ì „ì²´ ìŠ¤ìº” (vGridBtn1, ext1 ë“± í¬í•¨)
        if not fund_name:
            fund_name = _extract_kofia_name_from_xml(xml_bytes)

        out = pd.DataFrame({"Date": dates, code: price})
        if fund_name:
            out["fundNm"] = fund_name

        out[code] = pd.to_numeric(out[code], errors="coerce")

        out = out.dropna(subset=["Date"])
        mask = (out["Date"].dt.date >= pd.to_datetime(start).date()) & \
            (out["Date"].dt.date <= pd.to_datetime(end).date())
        out = out.loc[mask].sort_values("Date").reset_index(drop=True)

        cols = ["Date", code] + (["fundNm"] if "fundNm" in out.columns else [])
        return out[cols]

    except Exception as e:
        st.warning(f"KOFIA íŒŒì‹± ì‹¤íŒ¨({code}): {e}")
        return pd.DataFrame(columns=["Date"])

@st.cache_data(ttl=86400)
def pretty_label(symbol: str) -> str:
    """
    í‘œì‹œìš© ë¼ë²¨:
      - KOFIA ì½”ë“œ: ë³€í™˜ ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì˜ˆ: KR5370199261)
      - ì•¼í›„ í‹°ì»¤: shortName/longName â†’ ì—†ìœ¼ë©´ í‹°ì»¤
      - ë„ˆë¬´ ê¸¸ë©´ 20ì '...' ì²˜ë¦¬
    """
    s = (symbol or "").strip().upper()
    if not s:
        return ""

    # KOFIA ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ
    if is_kofia_code(s):
        return s

    # ì•¼í›„ í‹°ì»¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
    try:
        info = yf.Ticker(s).info or {}
    except Exception:
        info = {}
    name = info.get("shortName") or info.get("longName") or s
    return (name[:20] + "...") if len(name) > 20 else name

@st.cache_data(ttl=1200, show_spinner=False)
def fetch_prices_mixed(tickers: tuple, start, end, use_adjust=True) -> pd.DataFrame:
    """
    - ì•¼í›„ í‹°ì»¤ëŠ” fetch_yf_pricesë¡œ
    - KOFIA(í€ë“œì½”ë“œ: ISIN KRxxxxxxxxxx ë˜ëŠ” 4~6ìë¦¬ ë‹¨ì¶•ì½”ë“œ)ëŠ” fetch_kofia_nav_xmlë¡œ
    ì•ˆì „í•˜ê²Œ í•©ì³ì„œ ë°˜í™˜. ìµœì†Œí•œ ['Date'] ì»¬ëŸ¼ì€ í•­ìƒ ë³´ì¥.
    """
    if not tickers:
        return pd.DataFrame(columns=["Date"])

    kofia_like, yahoo_like = [], []
    for t in tickers:
        t = (t or "").strip()
        if not t:
            continue
        if is_kofia_code(t):
            kofia_like.append(t)
        else:
            yahoo_like.append(t)

    frames = []

    # 1) Yahoo
    if yahoo_like:
        yf_df = fetch_yf_prices(tuple(yahoo_like), start, end, use_adjust=use_adjust)
        if isinstance(yf_df, pd.DataFrame) and not yf_df.empty and "Date" in yf_df.columns:
            frames.append(yf_df)

    # 2) KOFIA
    for code in kofia_like:
        df_k = fetch_kofia_nav_xml(code, start, end)
        if isinstance(df_k, pd.DataFrame) and not df_k.empty and "Date" in df_k.columns:
            frames.append(df_k)
        else:
            st.info(f"KOFIA ë°ì´í„° ì—†ìŒ ë˜ëŠ” ì½”ë“œ í™•ì¸ í•„ìš”: {code}")

    if not frames:
        return pd.DataFrame(columns=["Date"])

    out = frames[0]
    for f in frames[1:]:
        if "Date" not in f.columns:
            continue
        # ğŸ”§ ì¶©ëŒ ë°©ì§€: fundNm ê°™ì€ ë©”íƒ€ì»¬ëŸ¼ì€ ì œê±°
        f = f.drop(columns=[c for c in f.columns if c.lower().startswith("fundnm")], errors="ignore")
        out = out.drop(columns=[c for c in out.columns if c.lower().startswith("fundnm")], errors="ignore")
        out = pd.merge(out, f, on="Date", how="outer")

    return out.sort_values("Date").reset_index(drop=True)

# -------------------- ì´ë¦„â†’í‹°ì»¤ ë³´í¸ ë³„ì¹­ --------------------
COMMON_ALIASES = {
    "nikkei225": "^N225", "nikkei 225": "^N225", "nikkei": "^N225",
    "shanghai": "000001.SS", "shanghai composite": "000001.SS",
    "kospi": "^KS11", "kosdaq": "^KQ11",
    "sp500": "^GSPC", "s&p500": "^GSPC", "dow": "^DJI", "nasdaq": "^IXIC",
    "eurostoxx50": "^STOXX50E", "euro stoxx 50": "^STOXX50E",
    "ftse100": "^FTSE", "hang seng": "^HSI", "dax": "^GDAXI", "cac40": "^FCHI",
    "dollar index": "DX-Y.NYB", "us dollar index": "DX-Y.NYB",
    "usdkrw": "USDKRW=X", "usdcny": "CNY=X",
}

# -------------------- Finviz í¬ë¡¤ëŸ¬ --------------------
@st.cache_data(ttl=3600, show_spinner=False)
def fetch_finviz_company(ticker: str):
    """
    Finvizì—ì„œ íšŒì‚¬ ì†Œê°œì™€ ìŠ¤ëƒ…ìƒ· ì§€í‘œ í…Œì´ë¸”ì„ ê°€ì ¸ì™€ (profile_text, df_wide) ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ (ë©”ì‹œì§€ ë¬¸ìì—´, ë‹¨ì¼ í–‰ DataFrame) ë°˜í™˜.
    """
    t = (ticker or "").strip().upper()
    if not t:
        return "í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", pd.DataFrame(columns=["Indicator 1","Value 1","Indicator 2","Value 2"])

    url = f"https://finviz.com/quote.ashx?t={t}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Referer": "https://finviz.com/",
        "Accept-Language": "en-US,en;q=0.9",
    }

    try:
        time.sleep(1.0)  # ìš”ì²­ ê°„ ë”œë ˆì´
        r = requests.get(url, headers=headers, timeout=10)
        if not r.ok:
            return "Finviz í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", pd.DataFrame(
                [{"Indicator 1":"Error","Value 1":"HTTP ì‹¤íŒ¨","Indicator 2":"URL","Value 2":url}]
            )

        html = r.text

        # íšŒì‚¬ ì†Œê°œ
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, "html.parser")
        node = soup.select_one('td.fullview-profile') or soup.select_one('td[class*="fullview-profile"]')
        profile_text = None
        if node:
            prof_raw = node.get_text(separator=" ", strip=True)
            profile_text = " ".join(ihtml.unescape(prof_raw).split())

        # Finvizì— ì—†ìœ¼ë©´ yfinance ë°±ì—…
        if not profile_text:
            try:
                info = (yf.Ticker(t).info or {})
                ysum = info.get("longBusinessSummary")
                if ysum:
                    profile_text = ysum.strip()
            except Exception:
                pass
        if not profile_text:
            profile_text = "íšŒì‚¬ ì†Œê°œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (Finviz)."

        # ìŠ¤ëƒ…ìƒ· í…Œì´ë¸”
        tables = pd.read_html(StringIO(html), attrs={"class": "snapshot-table2"})
        if not tables:
            df_wide = pd.DataFrame(
                [{"Indicator 1":"Error","Value 1":"ìŠ¤ëƒ…ìƒ· í‘œ ì—†ìŒ","Indicator 2":"Ticker","Value 2":t}]
            )
        else:
            snap = tables[0].copy()
            labels, values = [], []
            cols = list(snap.columns)
            for i in range(0, len(cols)-1, 2):
                labels += snap.iloc[:, i].astype(str).tolist()
                values += snap.iloc[:, i+1].astype(str).tolist()

            n = len(labels)
            rows = (n + 1) // 2
            data = []
            for k in range(rows):
                i1, i2 = 2*k, 2*k+1
                c1 = labels[i1] if i1 < n else ""
                v1 = values[i1] if i1 < n else ""
                c2 = labels[i2] if i2 < n else ""
                v2 = values[i2] if i2 < n else ""
                data.append([c1, v1, c2, v2])
            df_wide = pd.DataFrame(data, columns=["Indicator 1","Value 1","Indicator 2","Value 2"])

        return profile_text, df_wide

    except Exception as e:
        return f"Finviz ë¡œë”© ì‹¤íŒ¨: {e}", pd.DataFrame(
            [{"Indicator 1":"Error","Value 1":str(e),"Indicator 2":"Ticker","Value 2":t}]
        )

@st.cache_data(ttl=3600, show_spinner=False)
def fetch_naver_overview(sym: str):
    """
    ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ êµ­ë‚´ ì¢…ëª© ê°œìš” + ì£¼ìš”ì§€í‘œ í…Œì´ë¸”ì„ ê¸ì–´ì™€
    (profile_text, df_wide) í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    df_wide ì»¬ëŸ¼: ["Indicator 1","Value 1","Indicator 2","Value 2"]
    """
    from bs4 import BeautifulSoup

    code6 = _to_krx_code(sym)
    if not code6:
        return "êµ­ë‚´ ì¢…ëª© ì½”ë“œë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", pd.DataFrame(
            columns=["Indicator 1","Value 1","Indicator 2","Value 2"]
        )

    url = f"https://finance.naver.com/item/main.nhn?code={code6}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        r = requests.get(url, headers=headers, timeout=10)
        # ë„¤ì´ë²„ëŠ” EUC-KRì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì¸ì½”ë”© ì§€ì •
        r.encoding = r.apparent_encoding or "euc-kr"
        soup = BeautifulSoup(r.text, "html.parser")

        # ì´ë¦„/ê°„ë‹¨ ì„¤ëª…
        nm_node = soup.select_one("div.wrap_company h2 a") or soup.select_one("div.wrap_company h2")
        name = nm_node.get_text(" ", strip=True) if nm_node else code6
        corp_info = soup.select_one("div.corp_info")
        desc = corp_info.get_text(" ", strip=True) if corp_info else ""
        profile_text = f"**{name}** â€” {desc or 'ë„¤ì´ë²„ ê¸ˆìœµ ê°œìš”'}"

        # í…Œì´ë¸” â†’ (ì§€í‘œ, ê°’) ìŒ ìˆ˜ì§‘
        pairs = []
        for tr in soup.select("table tr"):
            ths = [th.get_text(" ", strip=True) for th in tr.select("th")]
            tds = [td.get_text(" ", strip=True) for td in tr.select("td")]
            for k in range(min(len(ths), len(tds))):
                lab, val = ths[k], tds[k]
                if lab and val and len(lab) <= 30:
                    pairs.append((lab, val))

        # ì¤‘ë³µ ë¼ë²¨ ì œê±°í•˜ê³  2ì—´ í‘œë¡œ ì¬êµ¬ì„±
        uniq, seen = [], set()
        for lab, val in pairs:
            if lab in seen:
                continue
            seen.add(lab); uniq.append((lab, val))

        rows = []
        for i in range(0, len(uniq), 2):
            c1, v1 = uniq[i]
            c2, v2 = uniq[i+1] if i+1 < len(uniq) else ("", "")
            rows.append([c1, v1, c2, v2])

        df_wide = pd.DataFrame(rows, columns=["Indicator 1","Value 1","Indicator 2","Value 2"])
        return profile_text, df_wide
    
    except Exception as e:
        return f"ë„¤ì´ë²„ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}", pd.DataFrame(
            columns=["Indicator 1","Value 1","Indicator 2","Value 2"]
        )

# === (NEW) ì™¸êµ­ì¸ ì§€ë¶„ìœ¨ ===
@st.cache_data(ttl=3600, show_spinner=False)
def fetch_naver_foreign_ratio(sym: str):
    """
    ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ í•œêµ­ ì¢…ëª©ì˜ ì™¸êµ­ì¸ ì§€ë¶„ìœ¨(%) ì¼ë³„ ì‹œê³„ì—´ì„ ìˆ˜ì§‘
    """
    from bs4 import BeautifulSoup
    code6 = _to_krx_code(sym)
    if not code6:
        return pd.DataFrame(columns=["Date","ForeignRatio"])

    url = f"https://finance.naver.com/item/frgn.nhn?code={code6}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        r.encoding = r.apparent_encoding or "euc-kr"
        tables = pd.read_html(r.text, header=0)
        if not tables:
            return pd.DataFrame(columns=["Date","ForeignRatio"])

        df = tables[1].copy()  # ë³´í†µ 2ë²ˆì§¸ í…Œì´ë¸”
        fr_col = next((c for c in df.columns if "ì™¸êµ­ì¸" in str(c)), None)
        date_col = next((c for c in df.columns if "ë‚ ì§œ" in str(c)), None)
        if not fr_col or not date_col:
            return pd.DataFrame(columns=["Date","ForeignRatio"])

        out = pd.DataFrame({
            "Date": pd.to_datetime(df[date_col], errors="coerce"),
            "ForeignRatio": pd.to_numeric(
                df[fr_col].astype(str).str.replace("%","").str.replace(",",""),
                errors="coerce"
            )
        }).dropna()
        return out.sort_values("Date").reset_index(drop=True)
    except Exception:
        return pd.DataFrame(columns=["Date","ForeignRatio"])    
    

# -------------------- ì•¼í›„ ê²€ìƒ‰ --------------------
def yahoo_search(query: str, quotes_count: int = 10):
    """ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ ê²€ìƒ‰(ë¹„ê³µì‹)"""
    q = query.strip()
    results = []
    alias_key = q.lower()
    if alias_key in COMMON_ALIASES:
        results.append({"symbol": COMMON_ALIASES[alias_key],
                        "shortname": f"Alias for '{q}'",
                        "longname": None, "exchDisp": "â€”", "quoteType": "ALIAS"})
    headers = {"User-Agent": "Mozilla/5.0", "Accept": "application/json, text/plain, */*"}
    url = "https://query2.finance.yahoo.com/v1/finance/search"
    try:
        r = requests.get(url, headers=headers,
                         params={"q": q, "quotesCount": quotes_count, "newsCount": 0},
                         timeout=8)
        if r.ok:
            js = r.json()
            for it in js.get("quotes", []):
                results.append({
                    "symbol": it.get("symbol"),
                    "shortname": it.get("shortname"),
                    "longname": it.get("longname") or it.get("longName"),
                    "exchDisp": it.get("exchDisp"),
                    "quoteType": it.get("quoteType"),
                })
    except Exception:
        pass
    # dedup
    seen, dedup = set(), []
    for x in results:
        sym = x.get("symbol")
        if not sym or sym in seen:
            continue
        seen.add(sym); dedup.append(x)
    return dedup[:quotes_count]

# -------------------- ê³µìš© ìœ í‹¸ --------------------
@st.cache_data(ttl=3600)
def load_base_data(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path, parse_dates=["Date"]).sort_values("Date")
    return df

@st.cache_data(ttl=600)
def load_meta(csv_path: str) -> str:
    try:
        m = pd.read_csv(csv_path)
        return str(m.iloc[0]["last_updated"])
    except Exception:
        return ""

@st.cache_data(ttl=1200, show_spinner=False)
def fetch_yf_prices(tickers: tuple, start, end, use_adjust=True) -> pd.DataFrame:
    if not tickers:
        return pd.DataFrame()
    raw = yf.download(
        list(tickers),
        start=str(start),
        end=str(end + pd.Timedelta(days=1)),  # endëŠ” ë°°íƒ€
        auto_adjust=use_adjust,
        progress=False,
        threads=True,
        actions=False,
    )
    if raw.empty:
        return pd.DataFrame()
    if isinstance(raw.columns, pd.MultiIndex):
        key = "Adj Close" if use_adjust and "Adj Close" in raw.columns.levels[0] else "Close"
        close = raw[key].copy()
    else:
        key = "Adj Close" if use_adjust and "Adj Close" in raw.columns else "Close"
        close = raw[[key]].copy()
        if len(tickers) == 1:
            close.columns = [list(tickers)[0]]
    close = close.sort_index()
    close.index.name = "Date"
    close = close.ffill().bfill(limit=1)
    return close.reset_index()

def reindex_fill_ffill_bfill(df: pd.DataFrame, start, end, ffill_limit: int = 5, kofia_ffill_limit: int = 5) -> pd.DataFrame:
    """
    ì˜ì—…ì¼ ê¸°ì¤€ìœ¼ë¡œ ë¦¬ì¸ë±ìŠ¤.
    - ì•¼í›„ ì—´: ì§§ê²Œ ffill (ê¸°ë³¸ 5ì¼)
    - KOFIA ì—´: ê³¼ë„í•œ ë‹¨ì ˆ ë°©ì§€ ìœ„í•´ ë™ì¼í•˜ê²Œ ffill (ê¸°ë³¸ 5ì¼)
      (bfillì€ ì ìš©í•˜ì§€ ì•ŠìŒ)
    """
    all_days = pd.bdate_range(start=start, end=end)
    out = df.set_index("Date").reindex(all_days)

    # ìˆ«ìí™”
    num_cols = [c for c in out.columns if c != "Date"]
    out[num_cols] = out[num_cols].apply(pd.to_numeric, errors="coerce")

    # êµ¬ë¶„
    kofia_cols = [c for c in out.columns if c != "Date" and is_kofia_code(str(c))]
    other_cols = [c for c in out.columns if c != "Date" and c not in kofia_cols]

    # ì•¼í›„: ì§§ê²Œ ffill
    if other_cols:
        out[other_cols] = out[other_cols].ffill(limit=ffill_limit)

    # KOFIA(í€ë“œ): ê³µë°± ì œê±°ìš© ffill (ì§§ì€ í•œë„ ë‚´)
    if kofia_cols:
        out[kofia_cols] = out[kofia_cols].ffill(limit=kofia_ffill_limit)

    return out.rename_axis("Date").reset_index()


def rebase_pct(frame: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    out = frame[["Date"] + cols].copy()
    for c in cols:
        s = out[c].dropna()
        out[c] = ((out[c] / s.iloc[0]) - 1.0) * 100.0 if not s.empty else pd.NA
    return out

def drawdown_pct(frame: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    out = frame[["Date"] + cols].copy()
    for c in cols:
        s = out[c].astype(float)
        peak = s.cummax()
        out[c] = (s / peak - 1.0) * 100.0
    return out

def format_tail_value(v, mode_key: str) -> str:
    if pd.isna(v): return ""
    return f"{v:+.1f}%" if mode_key in ("pct", "mdd") else f"{v:,.1f}"

# ==================== TAB 1: Market ====================
def tab_market():
    base = load_base_data(DATA_CSV)
    last_updated = load_meta(META_CSV)

    st.title("Market Performance")
    if last_updated:
        st.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸(KST): {last_updated} developed by W.I Lee")

    # UI CSS
    st.markdown("""
    <style>
    :root { --ctrl-h: 42px; --pad-y: 8px; }
    div.stTextInput, div[data-testid="stTextInput"],
    div.stDateInput, div[data-testid="stDateInput"] { min-height: var(--ctrl-h) !important; }
    div.stTextInput input, div[data-testid="stTextInput"] input,
    div.stDateInput input,  div[data-testid="stDateInput"] input {
      height: var(--ctrl-h) !important; line-height: var(--ctrl-h) !important;
      padding-top: var(--pad-y) !important; padding-bottom: var(--pad-y) !important;
    }
    div.stButton { min-height: var(--ctrl-h) !important; }
    div.stButton > button,
    div[data-testid="baseButton-primary"] button,
    div[data-testid="baseButton-secondary"] button,
    div[data-testid="baseButton-default"] button {
      height: var(--ctrl-h) !important; padding-top: var(--pad-y) !important; padding-bottom: var(--pad-y) !important;
    }
    div[data-testid="stWidgetLabel"] > label { margin-bottom: 4px !important; }
    [data-testid="stDataFrame"] table td, 
    [data-testid="stDataFrame"] table th { text-align: center !important; }
    </style>
    """, unsafe_allow_html=True)

    # ê¸°ê°„ ì„¤ì •
    min_d = base["Date"].min().date()
    max_d = base["Date"].max().date()
    default_start = max(date(2025, 1, 1), min_d)

    # â–¼ í•œ ì¤„ ë ˆì´ì•„ì›ƒ: ì‹œì‘ì¼ | ì¢…ë£Œì¼ | í‘œì‹œí†µí™” | í‹°ì»¤ì…ë ¥ | ë²„íŠ¼
    st.session_state.setdefault("m_tickers", "")
    col_s, col_e, col_ccy, col_inp, col_btn = st.columns([1.2, 1.2, 0.9, 3.2, 0.6])

    with col_s:
        start = st.date_input("ì‹œì‘ì¼", value=default_start,
                            min_value=date(2000, 1, 1), max_value=max_d, key="m_start")

    with col_e:
        end = st.date_input("ì¢…ë£Œì¼", value=max_d,
                            min_value=date(2000, 1, 1), max_value=max_d, key="m_end")

    with col_ccy:
        ccy = st.selectbox("í‘œì‹œí†µí™”", ["LOCAL", "KRW", "USD"], index=0, key="m_ccy")

    with col_inp:
        st.text_input("í‹°ì»¤/í€ë“œì½”ë“œ ì…ë ¥ (ì•¼í›„ + KOFIA í˜¼ìš© ê°€ëŠ¥)",
                    key="m_tickers",
                    placeholder="ì˜ˆ: SPY, ^KS11, 005930.KS, KR5370199261, 19926")

    with col_btn:
        st.markdown("<div style='height:26px'></div>", unsafe_allow_html=True)  # ë²„íŠ¼ ìˆ˜ì§ ì •ë ¬
        fetch_clicked = st.button("ë°˜ì˜", type="primary", use_container_width=True, key="m_fetch")

    # ì²´í¬ë°•ìŠ¤ëŠ” ë‹¤ìŒ ì¤„ë¡œ ë‘ì–´ ê³µê°„ í™•ë³´(ì›í•˜ë©´ ìœ„ col_ccyì— ë„£ì–´ë„ ë¨)
    use_adj = st.checkbox("ì¡°ì •ê°€ê²© ì‚¬ìš©(ë°°ë‹¹/ì•¡ë©´ ë°˜ì˜)", value=False, key="m_adj")

    # ê¸°ë³¸ CSV êµ¬ê°„
    mask = (base["Date"].dt.date >= start) & (base["Date"].dt.date <= end)
    view = base.loc[mask].copy()

    st.session_state.setdefault("m_extra", [])
    st.session_state.setdefault("m_ycols", [])

    def expand_aliases(seq):
        # ë³„ì¹­ë§Œ í™•ì¥, KOFIA ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ë‘ 
        out = []
        for t in seq:
            out.append(COMMON_ALIASES.get(t.lower(), t))
        return out

    # ì €ì¥ëœ ì¶”ê°€ ìì‚°
    saved = tuple(st.session_state["m_extra"])
    if saved:
        fetched_saved = fetch_prices_mixed(saved, start, end, use_adjust=use_adj)
        if not fetched_saved.empty:
            view = pd.merge(view, fetched_saved, on="Date", how="outer").sort_values("Date")

    
    # ì‹ ê·œ ì¶”ê°€
    new_only = []
    if fetch_clicked:
        raw_terms = [t for t in re.split(r"[,\s]+", st.session_state.get("m_tickers","")) if t.strip()]
        terms = expand_aliases(raw_terms)

        all_codes, seen = [], set()
        for t in terms:
            t_clean = t.strip().upper()

            # âœ… Market íƒ­ì—ì„œëŠ” í€ë“œëª… ê²€ìƒ‰ ê¸ˆì§€ â†’ ì •í™•í•œ ì½”ë“œë§Œ í—ˆìš©
            if t_clean in FUND_CODE_SET or is_kofia_code(t_clean):
                hits = [t_clean]
            else:
                # ì•¼í›„ í‹°ì»¤ í™•ì¸
                try:
                    hist = yf.Ticker(t_clean).history(period="1d")
                    hits = [t_clean] if not hist.empty else []
                except Exception:
                    hits = []

            for h in hits:
                if h not in seen:
                    all_codes.append(h)
                    seen.add(h)

        # 5) ì´ë¯¸ ìˆëŠ” ì»¬ëŸ¼/ì¶”ê°€ëª©ë¡ ì œì™¸ í›„ ìƒˆë¡œ ê°€ì ¸ì˜¤ê¸°
        already = set(view.columns) | set(st.session_state["m_extra"])
        new_only = [t for t in all_codes if t not in already]

    if new_only:
        with st.spinner(f"ê°€ê²© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... ({', '.join(new_only)})"):
            fetched_now = fetch_prices_mixed(tuple(new_only), start, end, use_adjust=use_adj)

        if not fetched_now.empty:
            drop_cols = [c for c in fetched_now.columns if c.lower().startswith("fundnm")]
            fetched_now = fetched_now.drop(columns=drop_cols, errors="ignore")

            drop_cols2 = [c for c in view.columns if c.lower().startswith("fundnm")]
            view = view.drop(columns=drop_cols2, errors="ignore")

            view = pd.merge(view, fetched_now, on="Date", how="outer").sort_values("Date")
            st.session_state["m_extra"] = sorted(set(st.session_state["m_extra"]) | set(new_only))
            st.session_state["m_ycols"] = list(
                dict.fromkeys(st.session_state.get("m_ycols", []) + new_only)
            )
            st.success(f"ì¶”ê°€ëœ ìì‚°: {', '.join(new_only)}")
        else:
            st.info("ìƒˆë¡œ ì¶”ê°€í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    # === í†µí™” ë³€í™˜ (LOCAL/KRW/USD) ===
    ccy = st.session_state.get("m_ccy", "LOCAL")
    if ccy in ("KRW", "USD"):
        usdkrw = None

        # 1) baseì— 'USDKRW'ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if "USDKRW" in view.columns:
            usdkrw = view[["Date", "USDKRW"]].dropna().set_index("Date")["USDKRW"]
        else:
            # 2) ì—†ìœ¼ë©´ ì•¼í›„ í™˜ìœ¨(KRW=X) ì¡°íšŒ
            fx = fetch_yf_prices(("KRW=X",), start, end, use_adjust=False)
            if not fx.empty and "KRW=X" in fx.columns:
                usdkrw = fx.set_index("Date")["KRW=X"]

        if usdkrw is not None and not usdkrw.empty:
            tmp = view.set_index("Date")

            # ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼(ìˆ«ìí˜•ë§Œ) â€” í™˜ìœ¨ ì»¬ëŸ¼ì€ ì œì™¸
            excl = {"USDKRW", "KRW=X"}
            cols = [c for c in tmp.columns if c not in excl]

            for c in cols:
                s = pd.to_numeric(tmp[c], errors="coerce")

                # í•œêµ­ ìì‚° íŒë³„: .KS/.KQ, KOFIA ì½”ë“œ, ëŒ€í‘œì§€ìˆ˜
                is_kr = (str(c).endswith(".KS") or str(c).endswith(".KQ")
                        or is_kofia_code(str(c)) or str(c) in ("^KS11", "^KQ11"))

                if ccy == "KRW" and not is_kr:
                    # í•´ì™¸/ë‹¬ëŸ¬ìì‚° â†’ ì›í™”
                    tmp[c] = s * usdkrw
                elif ccy == "USD" and is_kr:
                    # ì›í™”ìì‚° â†’ ë‹¬ëŸ¬
                    tmp[c] = s / usdkrw

            view = tmp.reset_index()
        else:
            st.info("í™˜ìœ¨(USDKRW / KRW=X) ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í•´ LOCALë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")
    
    # ë¦¬ì¸ë±ì‹±
    view = reindex_fill_ffill_bfill(view, start, end)

    all_cols = [c for c in view.columns if c != "Date"]
    init_default = all_cols[:min(3, len(all_cols))]
    st.session_state["m_ycols"] = [c for c in st.session_state.get("m_ycols", init_default) if c in all_cols] or init_default
    ycols = st.multiselect("í‘œì‹œí•  ìì‚°", options=all_cols, key="m_ycols", format_func=pretty_label_with_fund)

    if not ycols:
        st.info("í‘œì‹œí•  ìì‚°ì„ ì„ íƒí•˜ì„¸ìš”."); return

    MODE_LABELS = {"price": "ê°€ê²©", "pct": "ì¼ë°˜ë³€í™”ìœ¨(%)", "pct_log": "ë¡œê·¸ ë³€í™”ìœ¨(%)", "mdd": "ìµœëŒ€ ë‚™í­(MDD)"}
    mode = st.radio("í‘œì‹œ ë°©ì‹", options=list(MODE_LABELS.keys()), index=1,
                    horizontal=True, format_func=lambda k: MODE_LABELS[k], key="m_mode")
    st.markdown("<h5>(1) Return Chart</h5>", unsafe_allow_html=True)

    # ===== ìœ íš¨ì„± & ìˆ«ìí™” =====
    plot_df = view[["Date"] + ycols].copy()
    for c in ycols:
        plot_df[c] = pd.to_numeric(plot_df[c], errors="coerce")
    plot_df = plot_df.dropna(subset=ycols, how="all")
    if plot_df.empty:
        st.info("í‘œì‹œ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„/í‹°ì»¤ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
        return

       # ë¼ë²¨ ìœ ì¼í™”
    from collections import Counter
    ycols = [c for c in ycols if c in plot_df.columns]
    base_map = {c: pretty_label_with_fund(c) for c in ycols}
    cnt = Counter(base_map.values())
    unique_map, used = {}, set()
    for c in ycols:
        base_lbl = base_map[c]
        lbl = base_lbl
        k = 2
        while lbl in used:
            lbl = f"{base_lbl} ({c})[{k}]"; k += 1
        unique_map[c] = lbl; used.add(lbl)

    # ===== ë°ì´í„° ê°€ê³µ & ê·¸ë˜í”„ =====
    if mode == "price":
        plot_df_use = plot_df.copy()
        y_title = "ê°€ê²©ì§€ìˆ˜"
        plot_df_disp = plot_df_use.rename(columns=unique_map)
        ycols_disp = list(unique_map.values())  # â† ë¬´ì¡°ê±´ ë¼ë²¨ë§Œ ì‚¬ìš©
        fig = px.line(plot_df_disp, x="Date", y=ycols_disp, render_mode="svg")
        fig.update_traces(connectgaps=True)
        fig.update_yaxes(tickformat=",.1f")

    elif mode == "pct":
        plot_df_use = rebase_pct(plot_df, ycols)  # % ë‹¨ìœ„
        y_title = "ëˆ„ì  ìˆ˜ìµë¥  (%)"
        plot_df_disp = plot_df_use.rename(columns=unique_map)
        ycols_disp = [unique_map.get(c, c) for c in ycols]
        fig = px.line(plot_df_disp, x="Date", y=ycols_disp, render_mode="svg")
        fig.update_traces(connectgaps=True)
        fig.update_yaxes(ticksuffix="%", rangemode="tozero")

    elif mode == "pct_log":
        pct = rebase_pct(view, ycols).copy()
        for c in ycols:
            pct[c] = (pd.to_numeric(pct[c], errors="coerce") / 100.0) + 1.0  # ë°°ìˆ˜

        plot_df_disp = pct.rename(columns=unique_map)
        ycols_disp = [unique_map.get(c, c) for c in ycols]

        vals = plot_df_disp[ycols_disp].apply(pd.to_numeric, errors="coerce")
        y_show = [c for c in ycols_disp if (vals[c] > 0).any()]
        if not y_show:
            st.info("ë¡œê·¸ì¶•ì— í‘œì‹œí•  ìˆ˜ ìˆëŠ” ì‹œë¦¬ì¦ˆê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ë³€í™”ìœ¨(%)ë¡œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            return

        y_min = float(vals[y_show].min().min())
        y_max = float(vals[y_show].max().max())
        tick_candidates = [0.25, 0.5, 1, 2, 3, 4, 5, 10, 20, 50, 100]
        tickvals = [v for v in tick_candidates if v > 0 and y_min * 0.95 <= v <= y_max * 1.05] or [1]
        if 1 not in tickvals:
            tickvals = sorted(set(tickvals + [1]))
        ticktext = [f"{(v - 1) * 100:.0f}%" for v in tickvals]

        y_title = "ëˆ„ì  ìˆ˜ìµë¥  (%, ë¡œê·¸ ê°„ê²©)"

        fig = px.line(plot_df_disp, x="Date", y=y_show, render_mode="svg")
        fig.update_traces(connectgaps=True)
        fig.update_layout(
            margin=dict(l=10, r=130, t=10, b=10),
            height=520,
            yaxis_title=y_title,
            legend=dict(groupclick="togglegroup"),
            uirevision="mkt",
            xaxis_rangeslider_visible=False,
        )
        fig.update_yaxes(type="log", tickvals=tickvals, ticktext=ticktext)

        last_idx = vals.dropna(how="all").index[-1]
        lx = plot_df_disp.loc[last_idx, "Date"]
        for c in y_show:
            sc = plot_df_disp[["Date", c]].dropna()
            if sc.empty:
                continue
            v_last = float(sc.iloc[-1][c])          # ë°°ìˆ˜
            pct_last = (v_last - 1.0) * 100.0       # %
            fig.add_trace(
                go.Scatter(
                    x=[lx],
                    y=[v_last],
                    mode="markers+text",
                    text=[f"{pct_last:+.1f}%"],
                    textposition="middle right",
                    marker=dict(size=6),
                    showlegend=False,
                    hoverinfo="skip",
                    legendgroup=c,
                )
            )

        st.plotly_chart(
            fig,
            use_container_width=True,
            config={"scrollZoom": False, "doubleClick": "reset", "displaylogo": False},
        )
        return

    else:  # mdd
        plot_df_use = drawdown_pct(plot_df, ycols)
        y_title = "MDD (%, ë‚®ì„ìˆ˜ë¡ ì‹¬í•¨)"
        plot_df_disp = plot_df_use.rename(columns=unique_map)
        ycols_disp = [unique_map.get(c, c) for c in ycols]
        fig = px.line(plot_df_disp, x="Date", y=ycols_disp, render_mode="svg")
        fig.update_traces(connectgaps=True)
        fig.update_yaxes(ticksuffix="%", rangemode="tozero")

    # ê³µí†µ ë ˆì´ì•„ì›ƒ & ë§ˆì»¤ ë¼ë²¨
    fig.update_layout(
        margin=dict(l=10, r=130, t=10, b=10),
        height=520,
        yaxis_title=y_title,
        legend=dict(groupclick="togglegroup"),
        uirevision="mkt",
        xaxis_rangeslider_visible=False,
    )
    for tr in fig.data:
        tr.legendgroup = tr.name

    if mode == "mdd":
        for c in ycols_disp:
            s = plot_df_disp[c]
            if s.dropna().empty: continue
            idx_min = s.idxmin()
            fig.add_trace(go.Scatter(
                x=[plot_df_disp.loc[idx_min, "Date"]],
                y=[s.loc[idx_min]],
                mode="markers+text",
                text=[format_tail_value(s.loc[idx_min], "mdd")],
                textposition="bottom right",
                marker=dict(size=8),
                showlegend=False, hoverinfo="skip", legendgroup=c))
    else:
        last_row = plot_df_disp.dropna().iloc[-1] if not plot_df_disp.dropna().empty else None
        if last_row is not None:
            lx = last_row["Date"]
            for c in [cl for cl in plot_df_disp.columns if cl != "Date"]:
                sc = plot_df_disp[["Date", c]].dropna()
                if sc.empty: continue
                idx_max = sc[c].idxmax(); x_max = sc.loc[idx_max, "Date"]; y_max = sc.loc[idx_max, c]
                idx_min = sc[c].idxmin(); x_min = sc.loc[idx_min, "Date"]; y_min = sc.loc[idx_min, c]
                fig.add_trace(go.Scatter(x=[x_max], y=[y_max], mode="markers+text",
                                         text=[format_tail_value(y_max, mode)], textposition="top right", textfont=dict(color="blue"),
                                         marker=dict(size=4), showlegend=False, hoverinfo="skip", legendgroup=c))
                if idx_min != idx_max:
                    fig.add_trace(go.Scatter(x=[x_min], y=[y_min], mode="markers+text",
                                             text=[format_tail_value(y_min, mode)], textposition="bottom right", textfont=dict(color="red"),
                                             marker=dict(size=4, color="blue"), showlegend=False, hoverinfo="skip", legendgroup=c))
                is_last_extreme = (x_max == lx) or (x_min == lx)
                if not is_last_extreme:
                    v_last = sc.iloc[-1][c]
                    fig.add_trace(go.Scatter(x=[lx], y=[v_last], mode="markers+text",
                                             text=[format_tail_value(v_last, mode)], textposition="middle right", textfont=dict(color="black"),
                                             marker=dict(size=4, color="black"), showlegend=False, hoverinfo="skip", legendgroup=c))

    st.plotly_chart(fig, use_container_width=True,
                    config={"scrollZoom": False, "doubleClick": "reset", "displaylogo": False})

    # ---- (2) ê¸°ê°„ë³„ ìˆ˜ìµë¥  ìŠ¤ëƒ…ìƒ· ----
    st.markdown("<h5>(2) Periodic Return</h5>", unsafe_allow_html=True)
    price_df = view[["Date"] + ycols].copy()
    for c in ycols:
        price_df[c] = pd.to_numeric(price_df[c], errors="coerce")

    windows = [("1D",1), ("1W",5), ("1M",21), ("3M",63), ("6M",126), ("12M",252), ("36M",756)]
    rows = []
    for c in ycols:
        s = price_df[c].dropna()
        # â¬‡ï¸ ë³€ê²½: pretty_label â†’ pretty_label_with_fund (í€ë“œëª… 20ì â€¦ ì ìš©)
        row = {"ìì‚°": pretty_label_with_fund(c)}
        for name, d in windows:
            if not s.empty and len(s) > d:
                val = s.pct_change(d).iloc[-1] * 100.0
                row[f"R_{name}"] = f"{val:+.2f}%"
            else:
                row[f"R_{name}"] = ""
        rows.append(row)
    snap = pd.DataFrame(rows, columns=["ìì‚°"] + [f"R_{n}" for n,_ in windows])
    st.dataframe(
        snap,
        use_container_width=True,
        hide_index=True,
        column_config={
            "ìì‚°": st.column_config.TextColumn("Asset", width="large"),
            "R_1D": st.column_config.TextColumn("1D"),
            "R_1W": st.column_config.TextColumn("1W"),
            "R_1M": st.column_config.TextColumn("1M"),
            "R_3M": st.column_config.TextColumn("3M"),
            "R_6M": st.column_config.TextColumn("6M"),
            "R_12M": st.column_config.TextColumn("12M"),
            "R_36M": st.column_config.TextColumn("36M"),
        }
    )

    # ---- (3) Risk & Return Metrics ----
    st.markdown("<h5>(3) Risk & Return Metrics</h5>", unsafe_allow_html=True)

    rows = []
    ann = 252  # ì—°í™˜ì‚° ê¸°ì¤€ (ê±°ë˜ì¼ìˆ˜)

    for c in ycols:
        s = price_df[c].dropna()
        if s.empty:
            continue
        ret = s.pct_change().dropna()
        if ret.empty:
            continue

        # CAGR
        cagr = (s.iloc[-1] / s.iloc[0]) ** (ann / len(ret)) - 1.0
        vol = ret.std() * (ann ** 0.5)

        # Sharpe
        sharpe = cagr / vol if vol > 0 else float("nan")

        # Sortino (í•˜ë°© ë³€ë™ì„±)
        downside = ret[ret < 0]
        dstd = downside.std() * (ann ** 0.5) if not downside.empty else float("nan")
        sortino = cagr / dstd if dstd and dstd > 0 else float("nan")

        # MDD
        cum = (1 + ret).cumprod()
        mdd = (cum / cum.cummax() - 1.0).min()

        # Calmar
        calmar = cagr / abs(mdd) if mdd < 0 else float("nan")

        rows.append([
            pretty_label_with_fund(c),
            f"{cagr*100:.2f}%",
            f"{vol*100:.2f}%",
            f"{sharpe:.2f}",
            f"{sortino:.2f}",  # ğŸ”‘ ì¶”ê°€ëœ í•­ëª©
            f"{mdd*100:.2f}%",
            f"{calmar:.2f}"
        ])

    if rows:
        sumdf = pd.DataFrame(
            rows,
            columns=["Asset","CAGR","ì—°ë³€ë™ì„±","Sharpe","Sortino","MDD","Calmar"]
        )
        st.dataframe(sumdf, use_container_width=True, hide_index=True)

    # ---- (4) ë‹¨ì¼ ìì‚° ì´ë™í‰ê·  + ìº”ë“¤/ê±°ë˜ëŸ‰/RSI ----
    st.markdown("<h5>(4) Chart with Candlestick, SMA, Volume & RSI</h5>", unsafe_allow_html=True)

    options = [(pretty_label_with_fund(c), c) for c in ycols]  # (ë¼ë²¨, ì½”ë“œ)
    sel_idx = st.selectbox(
        "ìì‚° ì„ íƒ",
        options=list(range(len(options))),
        format_func=lambda i: options[i][0],
        index=0,
        key="m_ma_one_idx"
    )
    one = options[sel_idx][1]                 # ë‚´ë¶€ ì½”ë“œ
    one_label = pretty_label_with_fund(one)   # ë¼ë²¨

    # âœ… ë³„ì¹­ ì ìš©
    one_norm = COMMON_ALIASES.get(one.lower(), one)

    # ---- ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ----
    if is_kofia_code(one_norm):
        # âœ… í€ë“œ (KOFIA NAV)
        df_nav = fetch_kofia_nav_xml(one_norm, start, end)
        if df_nav.empty:
            st.warning("í•´ë‹¹ í€ë“œì˜ NAV ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame({
            "Date": df_nav["Date"],
            "Close": pd.to_numeric(df_nav[one_norm], errors="coerce")
        })
        df["Ticker"] = one_norm
        # High/Low/Volume ì—†ìŒ

    else:
        # âœ… ì£¼ì‹/ETF (Yahoo Finance OHLCV)
        ohlcv = fetch_yf_ohlcv((one_norm,), start, end, use_adjust=True)
        if ohlcv.empty:
            st.warning("í•´ë‹¹ ìì‚°ì˜ OHLCV ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        df = ohlcv.copy()

    # ---- ê³µí†µ ì§€í‘œ ê³„ì‚° (SMA, RSI) ----
    if "Close" in df.columns:
        for w in (20, 60, 120):
            df[f"SMA{w}"] = df["Close"].rolling(w).mean()

        def calc_RSI(series, period=14):
            delta = series.diff()
            up = delta.clip(lower=0)
            down = -1 * delta.clip(upper=0)
            roll_up = up.ewm(span=period, adjust=False).mean()
            roll_down = down.ewm(span=period, adjust=False).mean()
            RS = roll_up / roll_down
            return 100 - (100 / (1 + RS))

        df["RSI14"] = calc_RSI(df["Close"], 14)

    # ---- ì°¨íŠ¸ ìƒì„± ----
    fig = go.Figure()

    if {"Open","High","Low","Close"}.issubset(df.columns):
        # âœ… ì£¼ì‹/ETF â†’ ìº”ë“¤ ì°¨íŠ¸
        fig.add_trace(go.Candlestick(
            x=df["Date"],
            open=df["Open"], high=df["High"], low=df["Low"], close=df["Close"],
            increasing_line_color="red", decreasing_line_color="blue",
            name=one_label
        ))
        if "Volume" in df.columns:
            fig.add_trace(go.Bar(
                x=df["Date"], y=df["Volume"],
                name="Volume", yaxis="y2", opacity=0.4
            ))
        price_domain = [0.45, 1.0]
        vol_domain = [0.25, 0.4]

    else:
        # âœ… í€ë“œ â†’ ì„  ì°¨íŠ¸
        fig.add_trace(go.Scatter(
            x=df["Date"], y=df["Close"],
            mode="lines", name=f"{one_label} (Close)", line=dict(color="blue")
        ))
        price_domain = [0.3, 1.0]
        vol_domain = None

    # ---- ì´ë™í‰ê· ì„  ----
    for w in (20, 60, 120):
        if f"SMA{w}" in df.columns:
            fig.add_trace(go.Scatter(
                x=df["Date"], y=df[f"SMA{w}"],
                mode="lines", name=f"SMA{w}", line=dict(dash="dot")
            ))

    # ---- RSI ----
    if "RSI14" in df.columns:
        fig.add_trace(go.Scatter(
            x=df["Date"], y=df["RSI14"],
            mode="lines", name="RSI(14)",
            line=dict(color="purple", width=1.2),
            yaxis="y3"
        ))
        for lvl, clr in [(30, "blue"), (70, "red")]:
            fig.add_trace(go.Scatter(
                x=df["Date"], y=[lvl]*len(df),
                mode="lines", name=f"RSI {lvl}",
                line=dict(color=clr, dash="dash", width=1),
                yaxis="y3"
            ))

    # ---- ì™¸êµ­ì¸ ì§€ë¶„ìœ¨ ----
    if _is_krx_symbol(one_norm):
        fr_df = fetch_naver_foreign_ratio(one_norm)
        if not fr_df.empty:
            fig.add_trace(go.Scatter(
                x=fr_df["Date"], y=fr_df["ForeignRatio"],
                mode="lines", name="ì™¸êµ­ì¸ ì§€ë¶„ìœ¨(%)",
                line=dict(color="green", width=1.5, dash="dot"),
                yaxis="y4"
            ))

    # ---- ë ˆì´ì•„ì›ƒ ----
    layout_args = dict(
        title=f"{one_label} â€” Chart with SMA, Volume & RSI",
        xaxis=dict(rangeslider=dict(visible=False)),
        yaxis=dict(title="Price", domain=price_domain),
        yaxis3=dict(title="RSI", domain=[0.0, 0.2], range=[0,100], showgrid=True),
        height=850,
        margin=dict(l=10, r=120, t=30, b=20),
        legend=dict(orientation="v", yanchor="top", y=0.99, xanchor="left", x=1.02)
    )
    if vol_domain:
        layout_args["yaxis2"] = dict(title="Volume", domain=vol_domain, showgrid=False)

    # ğŸ”‘ ì™¸êµ­ì¸ ì§€ë¶„ìœ¨ yì¶• ì¶”ê°€
    if _is_krx_symbol(one_norm):
        layout_args["yaxis4"] = dict(
            title="ì™¸êµ­ì¸ ì§€ë¶„ìœ¨(%)", domain=[0.20,0.28],
            showgrid=False, overlaying="y", side="right"
        )

    fig.update_layout(**layout_args)
    st.plotly_chart(fig, use_container_width=True, key="market_chart_main")


    # ---- ë ˆì´ì•„ì›ƒ ----
    layout_args = dict(
        title=f"{one_label} â€” Chart with SMA, Volume & RSI",
        xaxis=dict(rangeslider=dict(visible=False)),
        yaxis=dict(title="Price", domain=price_domain),
        yaxis3=dict(title="RSI", domain=[0.0, 0.2], range=[0,100], showgrid=True),
        height=850,
        margin=dict(l=10, r=120, t=30, b=20),
        legend=dict(orientation="v", yanchor="top", y=0.99, xanchor="left", x=1.02)
    )
    if vol_domain:
        layout_args["yaxis2"] = dict(title="Volume", domain=vol_domain, showgrid=False)

    fig.update_layout(**layout_args)
    st.plotly_chart(fig, use_container_width=True)

    # ---- (5) Company / Fund Info ----
    st.markdown("<h5>(5) Company / Fund Info</h5>", unsafe_allow_html=True)

    # (3)ì—ì„œ ì„ íƒí•œ ìì‚°(one_norm)ê³¼ ì…ë ¥ì°½ stateë¥¼ ë™ê¸°í™” â†’ TSLAë¡œ ê³ ì •ë˜ëŠ” ë¬¸ì œ ë°©ì§€
    if st.session_state.get("m_info_symbol_src") != one_norm:
        st.session_state["m_info_symbol"] = one_norm
        st.session_state["m_info_symbol_src"] = one_norm

    c_fv1, c_fv2 = st.columns([0.22, 0.78])
    with c_fv1:
        # value= ì‚¬ìš© ê¸ˆì§€, stateë§Œ ì‚¬ìš©
        info_sym = st.text_input("í‹°ì»¤/ì½”ë“œ", key="m_info_symbol").strip().upper()
    with c_fv2:
        st.caption("êµ­ë‚´(.KS/.KQ ë˜ëŠ” 6ìë¦¬) â†’ ë„¤ì´ë²„, í€ë“œ(KRâ€¦, 4~6ìë¦¬ ë‹¨ì¶•) â†’ KOFIA, ê·¸ ì™¸ â†’ Finviz")

    if info_sym:
        try:
            # 1) KOFIA í€ë“œ
            if is_kofia_code(info_sym):
                nav_df = fetch_kofia_nav_xml(info_sym, start, end)
                if not nav_df.empty:
                    fund_nm = (nav_df.get("fundNm").dropna().iloc[0]
                            if "fundNm" in nav_df.columns and nav_df["fundNm"].notna().any()
                            else info_sym)
                    st.markdown(f"**{fund_nm}** â€” KOFIA ë“±ë¡ í€ë“œ")
                    tail = nav_df.dropna(subset=[info_sym]).tail(1)
                    if not tail.empty:
                        d_last = tail["Date"].dt.date.iloc[0]
                        v_last = float(tail[info_sym].iloc[0])
                        st.caption(f"ìµœê·¼ ê¸°ì¤€ê°€: {d_last} Â· {v_last:,.2f}")
                else:
                    st.info("KOFIA ê¸°ì¤€ê°€ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            # 2) êµ­ë‚´ ì£¼ì‹/ETF (.KS/.KQ ë˜ëŠ” 6ìë¦¬) â†’ ë„¤ì´ë²„
            elif info_sym.endswith((".KS", ".KQ")) or _is_krx_symbol(info_sym):
                profile_text, df_info = fetch_naver_overview(info_sym)
                st.markdown(profile_text)
                if not df_info.empty:
                    st.dataframe(df_info, use_container_width=True, hide_index=True, height=400)
                else:
                    st.caption("ë„¤ì´ë²„ í‘œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            # 3) ê·¸ ì™¸ â†’ Finviz
            else:
                with st.spinner("íšŒì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘..."):
                    profile_text, fv_table = fetch_finviz_company(info_sym)
                st.markdown(profile_text)
                if not fv_table.empty:
                    st.dataframe(fv_table, use_container_width=True, hide_index=True, height=400)
                else:
                    st.caption("í‘œì‹œí•  Key Metricsê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.warning(f"ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    else:
        st.info("ì‹¬ë³¼/ì½”ë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

    # ---- ë‹¤ìš´ë¡œë“œ ----
    st.markdown("#### ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    dl_df = (rebase_pct(view, ycols) if mode in ("pct","pct_log","mdd") else view)[["Date"] + ycols].copy()
    csv_key = f"mkt_csv_{mode}_{start}_{end}"
    xlsx_key = f"mkt_xlsx_{mode}_{start}_{end}"
    st.download_button(
        "CSV ë‹¤ìš´ë¡œë“œ",
        data=dl_df.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"market_{mode}_{start}_{end}.csv",
        mime="text/csv",
        key=csv_key,
    )
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="xlsxwriter") as w:
        dl_df.to_excel(w, sheet_name="market", index=False)
    bio.seek(0)
    st.download_button(
        "ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=bio.getvalue(),
        file_name=f"market_{mode}_{start}_{end}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key=xlsx_key,
    )

# ==================== TAB 2: Portfolio (ì›ë³¸ ìœ ì§€ + í˜¼í•©ì†ŒìŠ¤ ì§€ì›) ====================
def guess_currency(ticker: str) -> str:
    t = ticker.upper()
    if is_kofia_code(t):  # KOFIAëŠ” ê¸°ë³¸ KRWë¡œ ê°€ì •
        return "KRW"
    if t.endswith(".KS") or t.endswith(".KQ"):
        return "KRW"
    return "USD"

def rebalance_mask(dates: pd.DatetimeIndex, freq: str) -> pd.Series:
    s = pd.Series(dates, index=dates)
    if freq == "M":
        nxt, cur = s.shift(-1).dt.to_period("M"), s.dt.to_period("M")
    elif freq == "Q":
        nxt, cur = s.shift(-1).dt.to_period("Q"), s.dt.to_period("Q")
    else:
        nxt, cur = s.shift(-1).dt.to_period("Y"), s.dt.to_period("Y")
    return (cur != nxt).fillna(True)

def build_portfolio_equity_missing_aware(prices, weights, mode="BH", fee_bps=0.0, reb_freq="M") -> pd.Series:
    tickers = [c for c in prices.columns if c in weights]
    W = pd.Series({t: float(weights[t]) for t in tickers})
    W = W / W.sum() if W.sum() > 0 else W

    valid_rows = prices.notna().any(axis=1)
    if not valid_rows.any(): return pd.Series(dtype=float, name="Portfolio")
    prices = prices.loc[valid_rows.idxmax():]
    rets = prices.pct_change(fill_method=None)

    dates = prices.index; V = 1.0; equity = []
    def apply_cost(value, w_prev, w_tgt):
        turnover = float(abs(w_prev - w_tgt).sum()) / 2.0
        cost = (fee_bps / 10000.0) * turnover
        return value * (1.0 - cost)

    first_mask = prices.iloc[0].notna()
    w_curr = ((W[first_mask] / W[first_mask].sum()).reindex(W.index, fill_value=0.0).values
              if first_mask.any() else W.values)

    rmask = rebalance_mask(dates, reb_freq) if mode == "RB" else pd.Series(False, index=dates)

    for i, dt in enumerate(dates):
        r = rets.iloc[i].fillna(0.0)[tickers].values
        port_ret = float((w_curr * r).sum())
        V *= (1.0 + port_ret); equity.append(V)
        if (1.0 + port_ret) != 0:
            w_curr = w_curr * (1.0 + r) / (1.0 + port_ret)
        if rmask.iloc[i]:
            w_tgt = (W / W.sum()).values if W.sum()!=0 else W.values
            V = apply_cost(V, w_curr, w_tgt); w_curr = w_tgt.copy()

    eq = pd.Series(equity, index=dates, name="Portfolio"); eq.iloc[0] = 1.0
    return eq

def portfolio_metrics(equity: pd.Series) -> dict:
    ret = equity.pct_change(fill_method=None).dropna()
    if ret.empty: return {}
    ann = 252
    cagr = (equity.iloc[-1]) ** (ann / len(ret)) - 1.0
    vol = ret.std() * math.sqrt(ann)
    sharpe = cagr / vol if vol > 0 else float("nan")
    dd = (equity / equity.cummax() - 1.0); mdd = dd.min()
    calmar = cagr / abs(mdd) if mdd < 0 else float("nan")
    return {"CAGR": cagr, "Vol": vol, "Sharpe": sharpe, "MDD": mdd, "Calmar": calmar}

def tab_portfolio():
    st.title("Portfolio Analysis")

    c1, c2, c3, c4 = st.columns([1.2, 1.1, 1, 1])
    with c1:
        start = st.date_input("ì‹œì‘ì¼", value=date(2025,1,1),
                              min_value=date(2000,1,1), max_value=date.today(), key="p_start")
    with c2:
        end = st.date_input("ì¢…ë£Œì¼", value=date.today(),
                            min_value=date(2000,1,1), max_value=date.today(), key="p_end")
    with c3:
        base_ccy = st.selectbox("ê¸°ì¤€í†µí™”", ["USD", "KRW"], index=0, key="p_ccy")
    with c4:
        fee_bps = st.number_input("ê±°ë˜ë¹„ìš©(bps)", min_value=0.0, max_value=200.0, step=1.0, value=0.0, key="p_fee")

    c5, c6 = st.columns([1,1])
    with c5:
        rb_mode = st.selectbox("ë¦¬ë°¸ëŸ°ì‹±", ["ì—†ìŒ(ë°”ì´ì•¤í™€ë“œ)", "ë§¤ì›”", "ë¶„ê¸°", "ë§¤ë…„"], index=0, key="p_rbmode")
    with c6:
        bench = st.text_input("ë²¤ì¹˜ë§ˆí¬(ì˜µì…˜, ì˜ˆ: SPY, QQQ, ^GSPC)", value="SPY", key="p_bench")

    n1, n2, n3 = st.columns(3)
    with n1: name1 = st.text_input("í¬íŠ¸í´ë¦¬ì˜¤ 1 ì´ë¦„", value="í¬íŠ¸í´ë¦¬ì˜¤ 1ì•ˆ", key="p_name1")
    with n2: name2 = st.text_input("í¬íŠ¸í´ë¦¬ì˜¤ 2 ì´ë¦„", value="í¬íŠ¸í´ë¦¬ì˜¤ 2ì•ˆ", key="p_name2")
    with n3: name3 = st.text_input("í¬íŠ¸í´ë¦¬ì˜¤ 3 ì´ë¦„", value="í¬íŠ¸í´ë¦¬ì˜¤ 3ì•ˆ", key="p_name3")

    lite = st.checkbox("ê²½ëŸ‰ ëª¨ë“œ(ì£¼ê°„ ë¦¬ìƒ˜í”Œ)", value=False, help="ë¸Œë¼ìš°ì €ê°€ ëŠë¦¬ë©´ ì¼œ ë³´ì„¸ìš”.", key="p_lite")

    # ìµœì´ˆ 1íšŒë§Œ ê¸°ë³¸ê°’ ì„¸íŒ…
    if "weights_df" not in st.session_state:
        st.session_state["weights_df"] = pd.DataFrame([
            {"í‹°ì»¤":"SPY", "P1(%)":40.0, "P2(%)":33.0, "P3(%)":40.0},
            {"í‹°ì»¤":"QQQ", "P1(%)":40.0, "P2(%)":33.0, "P3(%)":40.0},
            {"í‹°ì»¤":"TLT", "P1(%)":20.0, "P2(%)":34.0, "P3(%)":20.0},
        ])

    with st.form("weights_form", clear_on_submit=False):
        h1, h2 = st.columns([1.0, 0.14])
        with h1:
            st.markdown("#### ìì‚° êµ¬ì„± (ê°€ë¡œ ì…ë ¥: 1/2/3ì•ˆ)")
        with h2:
            apply_weights = st.form_submit_button("ë°˜ì˜", use_container_width=True)

        # âš ï¸ ì—¬ê¸°ì„œëŠ” ì»¬ëŸ¼ ë¼ë²¨ ê³ ì • (ì ˆëŒ€ name1 ê°™ì€ ë³€ìˆ˜ ì“°ì§€ ë§ê¸°)
        edited = st.data_editor(
            st.session_state["weights_df"],
            num_rows="dynamic",
            use_container_width=True,
            key="p_table_fixed",
            column_config={
                "í‹°ì»¤": st.column_config.TextColumn("í‹°ì»¤ ë˜ëŠ” í€ë“œì½”ë“œ(KR..., 5~6ìë¦¬ ìˆ«ì)"),
                "P1(%)": st.column_config.NumberColumn("í¬íŠ¸í´ë¦¬ì˜¤1 (%)", step=1.0, format="%.2f"),
                "P2(%)": st.column_config.NumberColumn("í¬íŠ¸í´ë¦¬ì˜¤2 (%)", step=1.0, format="%.2f"),
                "P3(%)": st.column_config.NumberColumn("í¬íŠ¸í´ë¦¬ì˜¤3 (%)", step=1.0, format="%.2f"),
            },
        )

    if apply_weights:
        st.session_state["weights_df"] = edited.copy()
        st.success("ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.")

    # ì´í›„ ì¶œë ¥/ê·¸ë˜í”„ì—ì„œë§Œ ì´ë¦„ ë°˜ì˜
    st.write(f"ğŸ“Š í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ì´ë¦„: {name1}, {name2}, {name3}")

    edit_df = st.session_state["weights_df"].copy()

    up = st.file_uploader("CSV ì—…ë¡œë“œ(ì»¬ëŸ¼: í‹°ì»¤, P1(%), P2(%), P3(%))", type=["csv"], key="p_upload")
    if up:
        try:
            csvdf = pd.read_csv(up)
            need = {"í‹°ì»¤","P1(%)","P2(%)","P3(%)"}
            if need.issubset(csvdf.columns):
                st.session_state["weights_df"] = csvdf[list(need)].copy()
                edit_df = st.session_state["weights_df"]; st.info("ì—…ë¡œë“œëœ CSVë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                st.warning("CSVì— 'í‹°ì»¤, P1(%), P2(%), P3(%)' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        except Exception as e:
            st.warning(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")

    edit_df = edit_df.dropna(subset=["í‹°ì»¤"]).copy()
    edit_df["í‹°ì»¤"] = edit_df["í‹°ì»¤"].astype(str).str.upper().str.strip()

    w1 = {r["í‹°ì»¤"]: float(r["P1(%)"]) for _, r in edit_df.iterrows() if pd.notna(r["P1(%)"]) and r["P1(%)"]!=0}
    w2 = {r["í‹°ì»¤"]: float(r["P2(%)"]) for _, r in edit_df.iterrows() if pd.notna(r["P2(%)"]) and r["P2(%)"]!=0}
    w3 = {r["í‹°ì»¤"]: float(r["P3(%)"]) for _, r in edit_df.iterrows() if pd.notna(r["P3(%)"]) and r["P3(%)"]!=0}
    if not (w1 or w2 or w3):
        st.warning("ìµœì†Œ í•œ ê°œ ì•ˆì— í‹°ì»¤ì™€ ê°€ì¤‘ì¹˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”."); st.stop()

    for nm, w in [(name1,w1),(name2,w2),(name3,w3)]:
        if w:
            ssum = sum(w.values())
            if abs(ssum - 100.0) > 1e-6:
                st.caption(f"{nm} ê°€ì¤‘ì¹˜ í•©ê³„: {ssum:.1f}% â†’ ìë™ ì •ê·œí™”(í•© 100%)")

    tickers = tuple(sorted(set(list(w1.keys()) + list(w2.keys()) + list(w3.keys()))))
    with st.spinner(f"ê°€ê²© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... ({', '.join(tickers)})"):
        raw_px = fetch_prices_mixed(tickers, start, end, use_adjust=True)
    if raw_px.empty: st.warning("ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."); st.stop()

    starts = {}
    for col in [c for c in raw_px.columns if c != "Date"]:
        s = raw_px[["Date", col]].dropna()
        starts[col] = s["Date"].min().date() if not s.empty else None
    with st.expander("ê° ìì‚° ë°ì´í„° ì‹œì‘ì¼(ìƒì¥/ì„¤ì •ì¼ ìœ ì‚¬)"):
        info_df = pd.DataFrame({"í‹°ì»¤": list(starts.keys()),
                                "ë°ì´í„° ì‹œì‘ì¼": [str(starts[k]) if starts[k] else "-" for k in starts]})
        st.table(info_df)

    px_df = raw_px.copy()
    if st.session_state.get("p_lite", False):
        px_df = px_df.set_index("Date").resample("W-FRI").last().reset_index()

    usdkrw = None
    if "KRW=X" not in px_df.columns:
        fx = fetch_yf_prices(("KRW=X",), start, end, use_adjust=False)
        if not fx.empty:
            fx_df = fx.rename(columns={"KRW=X":"USDKRW"})
            if st.session_state.get("p_lite", False):
                fx_df = fx_df.set_index("Date").resample("W-FRI").last().reset_index()
            usdkrw = fx_df.set_index("Date")["USDKRW"]

    base_ccy = st.session_state.get("p_ccy", "USD")
    if usdkrw is not None:
        tmp = px_df.set_index("Date")
        for c in [c for c in tmp.columns if c != "Date"]:
            is_kr = c.endswith(".KS") or c.endswith(".KQ") or is_kofia_code(c)
            if base_ccy == "KRW" and not is_kr:
                tmp[c] = pd.to_numeric(tmp[c], errors="coerce") * usdkrw
            elif base_ccy == "USD" and is_kr:
                tmp[c] = tmp[c] / usdkrw
        px_df = tmp.reset_index()

    prices = px_df.set_index("Date")
    num_cols = [c for c in prices.columns if c != "Date"]
    prices[num_cols] = prices[num_cols].apply(pd.to_numeric, errors="coerce")
    first_valids = prices.apply(lambda s: s.first_valid_index())
    common_start = first_valids.dropna().max()
    if pd.notna(common_start):
        prices = prices.loc[common_start:]



    rb_mode = st.session_state.get("p_rbmode", "ì—†ìŒ(ë°”ì´ì•¤í™€ë“œ)")
    mode = "BH" if rb_mode.startswith("ì—†ìŒ") else "RB"
    freq = "M" if rb_mode.startswith("ë§¤ì›”") else ("Q" if rb_mode.startswith("ë¶„ê¸°") else "A")

    name1 = st.session_state.get("p_name1", "í¬íŠ¸í´ë¦¬ì˜¤ 1ì•ˆ")
    name2 = st.session_state.get("p_name2", "í¬íŠ¸í´ë¦¬ì˜¤ 2ì•ˆ")
    name3 = st.session_state.get("p_name3", "í¬íŠ¸í´ë¦¬ì˜¤ 3ì•ˆ")

    portfolios = []
    for nm, w in [(name1,w1),(name2,w2),(name3,w3)]:
        if not w:
            portfolios.append((nm, pd.Series(dtype=float))); continue
        W = pd.Series(w, dtype=float); W = W / (W.sum() if W.sum()!=0 else 1)
        eq = build_portfolio_equity_missing_aware(prices, W.to_dict(), mode=mode,
                                                  fee_bps=st.session_state.get("p_fee",0.0), reb_freq=freq)
        eq.name = nm; portfolios.append((nm, eq))

    bench = st.session_state.get("p_bench","SPY")
    bench_line = None
    bench_name = bench.strip().upper() if bench.strip() else None
    if bench_name:
        bpx = fetch_yf_prices((bench_name,), start, end, use_adjust=True)
        if not bpx.empty:
            if st.session_state.get("p_lite", False):
                bpx = bpx.set_index("Date").resample("W-FRI").last().reset_index()
            bser = bpx.set_index("Date")[bench_name]
            if usdkrw is not None:
                cur_kr = bench_name.endswith(".KS") or bench_name.endswith(".KQ")
                if base_ccy == "KRW" and not cur_kr: bser = bser * usdkrw
                elif base_ccy == "USD" and cur_kr: bser = bser / usdkrw
            bench_line = (bser / bser.dropna().iloc[0]).rename(bench_name)

    idx = None
    for _, s in portfolios:
        if s is not None and not s.empty:
            idx = s.index if idx is None else idx.union(s.index)
    if bench_line is not None:
        idx = bench_line.index if idx is None else idx.union(bench_line.index)
    if idx is None: idx = prices.index

    df_plot = pd.DataFrame(index=idx).sort_index()
    for nm, s in portfolios:
        if not s.empty:
            df_plot[nm] = (s / s.iloc[0] - 1.0) * 100.0
    if bench_line is not None:
        df_plot[bench_line.name] = (bench_line / bench_line.dropna().iloc[0] - 1.0) * 100.0
    df_plot = df_plot.reset_index().rename(columns={"index":"Date"})

    fig = px.line(df_plot, x="Date", y=[c for c in df_plot.columns if c != "Date"], render_mode="svg")
    fig.update_layout(margin=dict(l=10, r=110, t=10, b=10), height=480,
                      yaxis_title=f"ëˆ„ì  ìˆ˜ìµë¥  (%) â€” ê¸°ì¤€í†µí™”: {base_ccy}",
                      uirevision="pf1", xaxis_rangeslider_visible=False)
    fig.update_yaxes(ticksuffix="%")
    for tr in fig.data: tr.legendgroup = tr.name

    last = df_plot.dropna().iloc[-1]; lx = last["Date"]
    for c in df_plot.columns[1:]:
        sc = df_plot[["Date", c]].dropna()
        if sc.empty: continue
        v_last = sc.iloc[-1][c]
        fig.add_trace(go.Scatter(x=[lx], y=[v_last], mode="markers+text",
                                 text=[f"{v_last:+.1f}%"], textposition="middle right",
                                 marker=dict(size=6), showlegend=False, hoverinfo="skip", legendgroup=c))
        imin = sc[c].idxmin()
        x_min, y_min = sc.loc[imin, "Date"], sc.loc[imin, c]
        fig.add_trace(go.Scatter(x=[x_min], y=[y_min], mode="markers+text",
                                 text=[f"{y_min:+.1f}%"], textposition="bottom right",
                                 marker=dict(size=8), showlegend=False, hoverinfo="skip", legendgroup=c))
    st.plotly_chart(fig, use_container_width=True)

    comp = pd.DataFrame(index=idx).sort_index()
    for nm, s in portfolios:
        if not s.empty:
            comp[f"{nm} MDD(%)"] = (s / s.cummax() - 1.0) * 100.0
    if bench_line is not None:
        bench_equity = (1.0 + (bench_line - bench_line.dropna().iloc[0]) / bench_line.dropna().iloc[0]).reindex(comp.index).ffill()
        bench_equity.iloc[0] = 1.0
        comp[f"{bench_name} MDD(%)"] = (bench_equity / bench_equity.cummax() - 1.0) * 100.0

    comp = comp.reset_index().rename(columns={"index":"Date"})
    mdd_cols = [c for c in comp.columns if c != "Date"]
    if mdd_cols:
        fig2 = px.line(comp, x="Date", y=mdd_cols, render_mode="svg")
        fig2.update_layout(margin=dict(l=10, r=110, t=10, b=10), height=300, yaxis_title="MDD (%)",
                           uirevision="pf2", xaxis_rangeslider_visible=False)
        fig2.update_yaxes(ticksuffix="%", rangemode="tozero")
        for tr in fig2.data: tr.legendgroup = tr.name
        st.plotly_chart(fig2, use_container_width=True)

    st.markdown("#### ìš”ì•½ ì§€í‘œ")
    rows = []
    for nm, s in portfolios:
        if not s.empty:
            m = portfolio_metrics(s)
            rows.append([nm, f"{m['CAGR']*100:.2f}%", f"{m['Vol']*100:.2f}%", f"{m['Sharpe']:.2f}",
                         f"{m['MDD']*100:.2f}%", f"{m['Calmar']:.2f}"])
    if bench_line is not None:
        bench_eq = (bench_line / bench_line.dropna().iloc[0]).reindex(idx).ffill()
        bench_eq.iloc[0] = 1.0
        m = portfolio_metrics(bench_eq)
        rows.append([bench_name, f"{m['CAGR']*100:.2f}%", f"{m['Vol']*100:.2f}%", f"{m['Sharpe']:.2f}",
                     f"{m['MDD']*100:.2f}%", f"{m['Calmar']:.2f}"])
    if rows:
        sumdf = pd.DataFrame(rows, columns=["í¬íŠ¸í´ë¦¬ì˜¤/ë²¤ì¹˜","CAGR","ì—°ë³€ë™ì„±","Sharpe","MDD","Calmar"])
        st.table(sumdf)

    out = pd.DataFrame(index=idx).sort_index().rename_axis("Date").reset_index()
    for nm, s in portfolios:
        if not s.empty:
            out[f"{nm}_Value"] = s.reindex(out["Date"]).astype(float).values
            out[f"{nm}_Return(%)"] = (s.reindex(out["Date"]) / s.iloc[0] - 1.0).astype(float) * 100.0
            out[f"{nm}_Drawdown(%)"] = (s.reindex(out["Date"]) / s.cummax().reindex(out["Date"]) - 1.0).astype(float) * 100.0
    if bench_line is not None:
        bench_eq = (bench_line / bench_line.dropna().iloc[0]).reindex(idx).ffill()
        bench_eq.iloc[0] = 1.0
        out[f"{bench_name}_Value"] = bench_eq.reindex(out["Date"]).astype(float).values
        out[f"{bench_name}_Return(%)"] = (bench_eq.reindex(out["Date"]) / bench_eq.iloc[0] - 1.0).astype(float) * 100.0
        out[f"{bench_name}_Drawdown(%)"] = (bench_eq.reindex(out["Date"]) / bench_eq.cummax().reindex(out["Date"]) - 1.0).astype(float) * 100.0

    csv_key  = f"pf_csv_{base_ccy}_{st.session_state.get('p_start', date(2020,1,1))}_{st.session_state.get('p_end', date.today())}"
    xlsx_key = f"pf_xlsx_{base_ccy}_{st.session_state.get('p_start', date(2020,1,1))}_{st.session_state.get('p_end', date.today())}"

    st.download_button(
        "CSV ë‹¤ìš´ë¡œë“œ",
        data=out.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"portfolio_compare_{base_ccy}_{st.session_state.get('p_start', date(2020,1,1))}_{st.session_state.get('p_end', date.today())}.csv",
        mime="text/csv",
        key=csv_key,
    )

    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="xlsxwriter") as w:
        out.to_excel(w, sheet_name="portfolio", index=False)
    bio.seek(0)

    st.download_button(
        "ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=bio.getvalue(),
        file_name=f"portfolio_compare_{base_ccy}_{st.session_state.get('p_start', date(2020,1,1))}_{st.session_state.get('p_end', date.today())}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key=xlsx_key,
    )

# ==================== TAB 3: Analysis (ê¸€ë¡œë²ŒÂ·í€ë“œ í˜¼í•©) ====================
def _coerce_date(s: pd.Series) -> pd.Series:
    if s.dtype.kind in "M":
        return pd.to_datetime(s).dt.tz_localize(None)
    out = pd.to_datetime(s, errors="coerce", infer_datetime_format=True)
    if out.notna().any():
        return out.dt.tz_localize(None)
    s2 = s.astype(str).str.replace(".", "-", regex=False).str.strip()
    out2 = pd.to_datetime(s2, errors="coerce", format="%Y-%m-%d")
    return out2.dt.tz_localize(None)

def _to_number(x: pd.Series) -> pd.Series:
    if pd.api.types.is_numeric_dtype(x):
        return pd.to_numeric(x, errors="coerce")
    return pd.to_numeric(x.astype(str).str.replace(",", "", regex=False).str.replace(" ", "", regex=False),
                         errors="coerce")

@st.cache_data(ttl=1200, show_spinner=False)
def fetch_yf_ohlcv(tickers: tuple, start, end, use_adjust=True) -> pd.DataFrame:
    cols_out = ["Date","Ticker","Open","High","Low","Close","Volume"]
    if not tickers:
        return pd.DataFrame(columns=cols_out)

    raw = yf.download(
        list(tickers),
        start=str(start),
        end=str(end + pd.Timedelta(days=1)),
        auto_adjust=use_adjust,
        progress=False,
        threads=True,
        actions=False,
    )
    if raw.empty:
        return pd.DataFrame(columns=cols_out)

    if isinstance(raw.columns, pd.MultiIndex):
        def pick(k): 
            return raw[k] if k in raw.columns.levels[0] else pd.DataFrame(index=raw.index)
        open_ = pick("Open"); high = pick("High"); low = pick("Low")
        close = pick("Close"); vol = pick("Volume")
        open_.index.name = close.index.name = "Date"

        dfO = open_.reset_index().melt(id_vars="Date", var_name="Ticker", value_name="Open")
        dfC = close.reset_index().melt(id_vars="Date", var_name="Ticker", value_name="Close")
        dfH = high.reset_index().melt(id_vars="Date", var_name="Ticker", value_name="High")
        dfL = low.reset_index().melt(id_vars="Date", var_name="Ticker", value_name="Low")
        dfV = vol.reset_index().melt(id_vars="Date", var_name="Ticker", value_name="Volume")

        out = dfO.merge(dfC, on=["Date","Ticker"], how="left")\
                 .merge(dfH, on=["Date","Ticker"], how="left")\
                 .merge(dfL, on=["Date","Ticker"], how="left")\
                 .merge(dfV, on=["Date","Ticker"], how="left")
    else:
        tkr = list(tickers)[0]
        # âœ… í™˜ìœ¨/ì§€ìˆ˜ (Closeë§Œ ìˆëŠ” ê²½ìš° ë³´ì •)
        if "Close" in raw.columns and "Open" not in raw.columns:
            out = pd.DataFrame({
                "Date": raw.index,
                "Ticker": tkr,
                "Close": raw["Close"]
            }).reset_index(drop=True)
            return out   # ğŸ”‘ ì—¬ê¸°ì„œ í•¨ìˆ˜ ëëƒ„

        # âœ… ì¼ë°˜ ì£¼ì‹/ETF (OHLCV ëª¨ë‘ ìˆìŒ)
        out = pd.DataFrame({
            "Date": raw.index,
            "Ticker": tkr,
            "Open": raw.get("Open"),
            "High": raw.get("High"),
            "Low":  raw.get("Low"),
            "Close":raw.get("Close"),
            "Volume": raw.get("Volume"),
        })
    out["Date"] = pd.to_datetime(out["Date"], errors="coerce")
    return out.dropna(subset=["Date"]).sort_values(["Ticker","Date"]).reset_index(drop=True)

def build_pair_series(df_long: pd.DataFrame, t1: str, t2: str):
    p = df_long.pivot(index="Date", columns="Ticker", values="Close")
    if t1 not in p.columns or t2 not in p.columns:
        return pd.DataFrame()
    out = pd.DataFrame(index=p.index)
    out["ratio"] = p[t1] / p[t2]
    out["spread_pct"] = (p[t1]/p[t1].iloc[0] - p[t2]/p[t2].iloc[0]) * 100.0
    return out.dropna()

def compute_price_indicators(df_long: pd.DataFrame, ma_windows=(20,60,120), vol_window=60):
    if df_long.empty:
        return {"data": df_long.copy()}

    def f(g):
        g = g.sort_values("Date").copy()
        g["ret"] = g["Close"].pct_change(fill_method=None)
        g["vol_ann"] = g["ret"].rolling(vol_window).std() * (252 ** 0.5)
        base = g["Close"] / g["Close"].iloc[0]
        g["MDD"] = (base / base.cummax() - 1.0)
        if {"High","Low","Close"}.issubset(g.columns):
            prev_close = g["Close"].shift(1)
            tr = pd.concat([(g["High"]-g["Low"]).abs(),
                            (g["High"]-prev_close).abs(),
                            (g["Low"]-prev_close).abs()], axis=1).max(axis=1)
            g["ATR"] = tr.rolling(14).mean()
            g["ATR_pct"] = g["ATR"] / g["Close"]
        else:
            g["ATR_pct"] = pd.NA
        if "Volume" in g.columns and g["Volume"].notna().any():
            d = g["ret"].fillna(0.0).apply(lambda x: 1 if x>0 else (-1 if x<0 else 0))
            g["OBV"] = (d * g["Volume"].fillna(0)).cumsum()
            pv = (g["Close"] * g["Volume"].fillna(0)).astype(float)
            g["PV_Z"] = (pv - pv.rolling(60).mean()) / (pv.rolling(60).std() + 1e-12)
        else:
            g["OBV"] = pd.NA; g["PV_Z"] = pd.NA
        return g

    try:
        data = (
            df_long.groupby("Ticker", group_keys=False)
            .apply(lambda g: f(g).assign(Ticker=g.name), include_groups=False)
            .reset_index(drop=True)
        )
    except TypeError:
        data = (
            df_long.groupby("Ticker", group_keys=False)
            .apply(lambda g: f(g).assign(Ticker=g.name))
            .reset_index(drop=True)
        )
    return {"data": data}

@st.cache_data(ttl=900, show_spinner=False)
def fetch_prices_mixed_long(tokens: tuple, start, end, use_adjust=True) -> pd.DataFrame:
    """
    ì•¼í›„ OHLCV(long) + KOFIA Close(long) ê²°í•© â†’ columns: Date, Ticker, Close, High, Low, Volume
    KOFIAëŠ” Closeë§Œ ì±„ì›Œì§(High/Low/Volumeì€ NaN).
    """
    cols_out = ["Date", "Ticker", "Close", "High", "Low", "Volume"]
    if not tokens:
        return pd.DataFrame(columns=cols_out)

    yf_list   = [t for t in tokens if not is_kofia_code(t)]
    fund_list = [t for t in tokens if is_kofia_code(t)]

    out_frames = []

    # 1) Yahoo OHLCV (long)
    if yf_list:
        out_frames.append(fetch_yf_ohlcv(tuple(yf_list), start, end, use_adjust=use_adjust))

    # 2) KOFIA NAV (Closeë§Œ)
    for code in fund_list:
        try:
            df = fetch_kofia_nav_xml(code, start, end)

            # ë¹„ê±°ë‚˜ Date ìì²´ë„ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
            if df is None or df.empty or "Date" not in df.columns:
                continue

            # ê°€ê²© ì»¬ëŸ¼ ê²°ì •: ë³´í†µ code, ì—†ìœ¼ë©´ Date ì œì™¸ ì²« ì»¬ëŸ¼
            price_col = code if code in df.columns else next(
                (c for c in df.columns if c != "Date"), None
            )
            if not price_col:
                st.info(f"KOFIA {code}: ê°€ê²© ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœ€ (cols={list(df.columns)})")
                continue

            # í‘œì¤€ ì»¬ëŸ¼ êµ¬ì„±
            tmp = df[["Date", price_col]].copy()
            tmp = tmp.rename(columns={price_col: "Close"})
            tmp["Date"] = pd.to_datetime(tmp["Date"], errors="coerce")
            tmp["Ticker"] = code
            tmp["High"] = pd.NA
            tmp["Low"] = pd.NA
            tmp["Volume"] = pd.NA

            out_frames.append(tmp[["Date", "Ticker", "Close", "High", "Low", "Volume"]])

        except Exception as e:
            st.warning(f"KOFIA ì¡°íšŒ ì‹¤íŒ¨({code}): {e}")


    if not out_frames:
        return pd.DataFrame(columns=cols_out)

    out = pd.concat(out_frames, ignore_index=True, sort=False)
    out = out[cols_out].copy()
    out["Date"] = pd.to_datetime(out["Date"], errors="coerce")

    num_cols = [c for c in out.columns if c not in ("Date","Ticker")]
    out[num_cols] = out[num_cols].apply(pd.to_numeric, errors="coerce")

    return out.dropna(subset=["Date"]).sort_values(["Ticker", "Date"]).reset_index(drop=True)

def tab_research_global():
    st.title("ê°€ê²© ë¦¬ì„œì¹˜ (ê¸€ë¡œë²ŒÂ·í€ë“œ í¬í•¨)")

    c1, c2, c3 = st.columns([1.6, 1.0, 1.0])
    with c1:
        raw = st.text_input("í‹°ì»¤/í€ë“œì½”ë“œ(ì‰¼í‘œ/ê³µë°± êµ¬ë¶„) â€” ì˜ˆ: 005930.KS, 069500.KS, SPY, ^N225, KR5370199261, 19926",
                            value="069500.KS, SPY, ^KS11")
    with c2:
        start = st.date_input("ì‹œì‘ì¼", value=date(2021,1,1), min_value=date(2000,1,1), max_value=date.today())
    with c3:
        end = st.date_input("ì¢…ë£Œì¼", value=date.today(), min_value=date(2000,1,1), max_value=date.today())

    use_adj = st.checkbox("ì¡°ì •ê°€ê²©(ë°°ë‹¹/ë¶„í•  ë°˜ì˜) ì‚¬ìš©(ì•¼í›„ì—ë§Œ í•´ë‹¹)", value=True)
    parsed = [t for t in re.split(r"[,\s]+", raw.upper()) if t.strip()]

    st.markdown("**í€ë“œ NAV íŒŒì¼ ì—…ë¡œë“œ(CSV/XLSX)** â€” ì»¬ëŸ¼ ì˜ˆì‹œ: `ì¼ì, ê¸°ì¤€ê°€` ë˜ëŠ” `Date, Close`")
    fund_files = st.file_uploader("ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ ê°€ëŠ¥", type=["csv","xls","xlsx"], accept_multiple_files=True)

    with st.spinner("ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        yf_fd_long = fetch_prices_mixed_long(tuple(parsed), start, end, use_adjust=use_adj) if parsed else \
                     pd.DataFrame(columns=["Date","Ticker","Close","High","Low","Volume"])

        # ì—…ë¡œë“œ íŒŒì¼ì€ Tickerë¥¼ íŒŒì¼ëª…ìœ¼ë¡œ ë¶€ì—¬
        extra_frames = []
        for f in fund_files or []:
            try:
                if f.name.lower().endswith(".csv"):
                    df = pd.read_csv(f)
                else:
                    df = pd.read_excel(f)
                # ìœ ì—°í•œ ì»¬ëŸ¼ëª… ì²˜ë¦¬
                cols = {c.strip(): c for c in df.columns}
                date_col = next((cols[c] for c in cols if c.lower() in ("date","ì¼ì","ê¸°ì¤€ì¼","ê¸°ì¤€ì¼ì")), None)
                close_col = next((cols[c] for c in cols if c.lower() in ("close","ê¸°ì¤€ê°€","nav","ê°€ê²©")), None)
                if date_col and close_col:
                    tmp = df[[date_col, close_col]].copy()
                tmp.columns = ["Date","Close"]
                tmp["Date"] = pd.to_datetime(tmp["Date"], errors="coerce")
                tmp["Close"] = pd.to_numeric(tmp["Close"], errors="coerce")
                tmp["Ticker"] = os.path.splitext(os.path.basename(f.name))[0].upper()
                tmp["High"] = pd.NA; tmp["Low"] = pd.NA; tmp["Volume"] = pd.NA
                extra_frames.append(tmp[["Date","Ticker","Close","High","Low","Volume"]])

            except Exception as e:
                st.warning(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨({f.name}): {e}")

    frames = [df for df in [yf_fd_long] + extra_frames if df is not None and not df.empty]
    base_long = (pd.concat(frames, ignore_index=True)
                 if frames else pd.DataFrame(columns=["Date","Ticker","Close","High","Low","Volume"]))
    if base_long.empty:
        st.warning("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í‹°ì»¤/íŒŒì¼/ê¸°ê°„ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    data = compute_price_indicators(base_long)["data"]

    tB, tC, tD = st.tabs(["âš ï¸ ë³€ë™ì„±Â·MDDÂ·ATR/OBV", "ğŸ”— ìƒê´€Â·í˜ì–´", "ğŸ§ª ì‹œë‚˜ë¦¬ì˜¤"])

    with tB:
        all_tickers = sorted(data["Ticker"].unique())
        sel2 = st.multiselect("ìì‚° ì„ íƒ", options=all_tickers,
                              default=all_tickers[:min(4,len(all_tickers))], key="rgB_sel")
        if sel2:
            disp = data[data["Ticker"].isin(sel2)].copy()
            fig1 = px.line(disp, x="Date", y="vol_ann", color="Ticker",
                           title="ë¡¤ë§ ë³€ë™ì„±(ì—°ìœ¨í™”, ì°½=60)", render_mode="svg")
            fig1.update_layout(height=300, margin=dict(l=10,r=110,t=30,b=10),
                               uirevision="rgB1", xaxis_rangeslider_visible=False)
            st.plotly_chart(fig1, use_container_width=True)

            disp["MDD(%)"] = disp["MDD"] * 100.0
            fig2 = px.line(disp, x="Date", y="MDD(%)", color="Ticker",
                           title="MDD(%) â€” ë‚®ì„ìˆ˜ë¡ ë‚™í­ í¼", render_mode="svg")
            fig2.update_layout(height=300, margin=dict(l=10,r=110,t=30,b=10),
                               uirevision="rgB2", xaxis_rangeslider_visible=False)
            fig2.update_yaxes(ticksuffix="%", rangemode="tozero")
            st.plotly_chart(fig2, use_container_width=True)

            if "ATR_pct" in disp and disp["ATR_pct"].notna().any():
                disp["ATR%(%)"] = disp["ATR_pct"] * 100.0
                fig3 = px.line(disp, x="Date", y="ATR%(%)", color="Ticker",
                               title="ATR% (ê°€ê²© ëŒ€ë¹„ í‰ê·  ì§„í­)", render_mode="svg")
                fig3.update_layout(height=280, margin=dict(l=10,r=110,t=30,b=10),
                                   uirevision="rgB3", xaxis_rangeslider_visible=False)
                fig3.update_yaxes(ticksuffix="%")
                st.plotly_chart(fig3, use_container_width=True)
            else:
                st.caption("High/Lowê°€ ì—†ëŠ” ìì‚°ë§Œ ì„ íƒë˜ì–´ ATR%ëŠ” ìƒëµë¨.")

            if "OBV" in disp and disp["OBV"].notna().any():
                voltab1, voltab2 = st.tabs(["OBV", "PriceÃ—Volume Z-score"])
                with voltab1:
                    fig4 = px.line(disp, x="Date", y="OBV", color="Ticker", title="OBV", render_mode="svg")
                    fig4.update_layout(height=260, margin=dict(l=10,r=110,t=30,b=10),
                                       uirevision="rgB4", xaxis_rangeslider_visible=False)
                    st.plotly_chart(fig4, use_container_width=True)
                with voltab2:
                    fig5 = px.line(disp, x="Date", y="PV_Z", color="Ticker", title="PV Z-score (ì°½=60)", render_mode="svg")
                    fig5.update_layout(height=260, margin=dict(l=10,r=110,t=30,b=10),
                                       uirevision="rgB5", xaxis_rangeslider_visible=False)
                    st.plotly_chart(fig5, use_container_width=True)
            else:
                st.caption("ì„ íƒ ìì‚°ì— ê±°ë˜ëŸ‰(Volume)ì´ ì—†ì–´ ê±°ë˜ëŒ€ê¸ˆ ì§€í‘œëŠ” ìˆ¨ê¹€ ì²˜ë¦¬ë¨.")

    with tC:
        pvt_ret = data.pivot(index="Date", columns="Ticker", values="ret").dropna(how="all")
        win = st.slider("ìƒê´€ê³„ìˆ˜ ìœˆë„ìš°(ê±°ë˜ì¼)", 30, 252, 120, 10)
        cor = pvt_ret.tail(win).corr().round(2) if not pvt_ret.empty else pd.DataFrame()
        st.markdown("**ìƒê´€í–‰ë ¬ (ìµœê·¼ ìœˆë„ìš°)**")
        st.dataframe(cor)

        c1, c2 = st.columns(2)
        with c1:
            t1 = st.selectbox("ìì‚° 1", options=sorted(data["Ticker"].unique()), key="pair_t1")
        with c2:
            t2 = st.selectbox("ìì‚° 2", options=[x for x in sorted(data["Ticker"].unique()) if x != t1], key="pair_t2")
        pair = build_pair_series(base_long, t1, t2)
        if not pair.empty:
            fig6 = px.line(pair.reset_index(), x="Date", y="ratio", title=f"ë¹„ìœ¨ {t1}/{t2}", render_mode="svg")
            fig6.update_layout(height=280, margin=dict(l=10,r=110,t=30,b=10),
                               uirevision="rgC1", xaxis_rangeslider_visible=False)
            st.plotly_chart(fig6, use_container_width=True)

            fig7 = px.line(pair.reset_index(), x="Date", y="spread_pct", title=f"ìŠ¤í”„ë ˆë“œ(ë¦¬ë² ì´ìŠ¤%) {t1} vs {t2}", render_mode="svg")
            fig7.update_layout(height=280, margin=dict(l=10,r=110,t=30,b=10),
                               uirevision="rgC2", xaxis_rangeslider_visible=False)
            fig7.update_yaxes(ticksuffix="%")
            st.plotly_chart(fig7, use_container_width=True)
        else:
            st.info("ì„ íƒí•œ ë‘ ìì‚°ì˜ ê°€ê²© ë°ì´í„°ë¥¼ ë™ì‹œì— ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    with tD:
        st.markdown("**ê°€ê²© ì¶©ê²©(Â±%) ì‹œë‚˜ë¦¬ì˜¤ â€” ì¦‰ì‹œ ì†ìµ ê³„ì‚°**")
        uniq = sorted(data["Ticker"].unique())
        shocks = {}
        cols = st.columns(min(4, max(1,len(uniq))))
        for i, t in enumerate(uniq):
            with cols[i % len(cols)]:
                shocks[t] = st.slider(f"{t} ì¶©ê²©(%)", -30, 30, 0, 1) / 100.0
        if shocks:
            show = pd.DataFrame({"Ticker": list(shocks.keys()),
                                 "ì¦‰ì‹œì†ìµ(%)": [f"{v*100:+.1f}%" for v in shocks.values()]})
            st.dataframe(show.set_index("Ticker"))

# -------------------- ìƒë‹¨ ê²€ìƒ‰ë°” --------------------
st.markdown("""
<style>
  .gs-row .stButton>button { height: 32px; padding: 4px 12px; }
  .gs-row div[data-testid="stTextInput"] input { height: 32px; padding: 4px 10px; }
  div[data-testid="stHorizontalBlock"] { margin-bottom: 0.25rem !important; }
</style>
""", unsafe_allow_html=True)

with st.container():
    c_label, c_input, c_btn = st.columns([0.10, 0.70, 0.12], gap="small")
    with c_label:
        st.markdown("**í‹°ì»¤ê²€ìƒ‰**")
    with c_input:
        q_global = st.text_input(
            "í‹°ì»¤ê²€ìƒ‰",
            key="g_search_query",
            placeholder="ì˜ˆ: SPY, ^KS11, 005930.KS, kospi",
            label_visibility="collapsed",
        )
    with c_btn:
        search_clicked = st.button("ê²€ìƒ‰", key="g_search_btn", use_container_width=True)

if search_clicked and q_global.strip():
    with st.spinner("ì•¼í›„ì—ì„œ ê²€ìƒ‰ ì¤‘..."):
        results = yahoo_search(q_global, quotes_count=10)
    if not results:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.caption("ê²€ìƒ‰ ê²°ê³¼")
        for item in results:
            sym  = item.get("symbol", "")
            name = item.get("shortname") or item.get("longname") or ""
            exch = item.get("exchDisp") or ""
            qt   = item.get("quoteType") or ""
            st.markdown(f"**{sym}** â€” {name} Â· {exch} Â· {qt}")

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["Market", "Portfolio", "Analysis"])
with tab1:  tab_market()
with tab2:  tab_portfolio()
with tab3:  tab_research_global()

# ê¹ƒí—ˆë¸Œ ì‚¬ì´íŠ¸
# https://github.com/anfwonil/investment4us

# ë°°í¬
# cd C:\Users\woori\Desktop\top10
# git add -A
# git commit -m "chore: update data and backup folders"
# git push

#   
# git commit -m "update: latest changes from run_update"
# git push origin main
   



# git add requirements.txt
# git commit -m "chore: update requirements.txt (add lxml)"

# ì‹¤í–‰ ì°¸ê³ :   KR5370199261
# cd C:\Users\woori\Desktop\top10
# & "C:\Users\woori\anaconda3\python.exe" -m streamlit run ".\app.py"